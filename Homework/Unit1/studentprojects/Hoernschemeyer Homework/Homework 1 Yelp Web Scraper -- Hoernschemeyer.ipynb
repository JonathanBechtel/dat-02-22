{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit 1 Homework:  Scraping the Yelp Website\n",
    "\n",
    "Welcome!  For this homework assignment you'll be tasked with building a web scraper in a manner that builds on what was covered in our web scraping class.\n",
    "\n",
    "The assignment will extend the lab work done during that time, where we built a dataset that listed the name, number of reviews and price range for restaurant on the following web page: https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1\n",
    "\n",
    "**What You'll Turn In:**\n",
    "\n",
    "A finished jupyter notebook that walks us through the steps you took in order to get your results.  Provide notes where appropriate to explain what you are doing.\n",
    "\n",
    "The notebook should produce a finished dataset at the end.  \n",
    "\n",
    "If for some reason you're experiencing problems with the final result, please let someone know when turning it in.\n",
    " \n",
    "The homework is divided into five tiers, each of which have increasing levels of difficulty:\n",
    "\n",
    "##### Tier 1: Five Columns From the First Page\n",
    "\n",
    "At the most basic level for this assignment, you will need to extend what we did in class, and create a dataset that has five columns in it that are 30 rows long.  This means you will not need to go off the first page in order to complete this section.\n",
    "\n",
    "##### Tier 2:  100 Row Dataset With At Least 3 Columns\n",
    "\n",
    "For this portion of the assignment, take 3 of your columns from step 1, and extend them out to multiple pages on the yelp website.  You should appropriately account for the presence of missing values.\n",
    "\n",
    "##### Tier 3:  100 Row Dataset With At Least 5 Columns\n",
    "\n",
    "Very similar to Tier 2, but if you use this many columns you will be forced to encounter some columns that will frequently have missing values, whereas with Tier 2 you could likely skip these if you wanted to.  \n",
    "\n",
    "##### Tier 4:  100 Row Dataset With At Least 5 Columns + Individual Restaurant Categories\n",
    "\n",
    "Restaurants often have different categories associated with them, so grabbing them individually as separate values is often challenging.  To complete this tier, you'll have to find a way to 'pick out' each of the individual categories as their own separate column value.  \n",
    "\n",
    "##### Tier 5:  Unlimited Row Dataset With At Least 5 Columns + Individual Restaurant Categories\n",
    "\n",
    "Take what you did in Tier 4, and extend it so that the code will work with an arbitrary number of pages.  Ie, regardless of how many pages there are listing the best restaurants in London, your scraper will find them, and cleanly parse their information into clean datasets.\n",
    "\n",
    "### Hints\n",
    "\n",
    "Here are a few tips that will save you time when completing this assignment:\n",
    "\n",
    " - The name, average rating, total ratings and neighborhood of a restaurant tend to be the 'easy' ones, because they rarely have missing values, so what ever logic you use on the first page will typically apply to all pages.  They are a good place to start\n",
    " - Phone numbers, price ranges and reviews are more commonly missing, so if you are trying to get a larger number of items from them across multiple pages you should expect to do some error handling\n",
    " - You can specify any sort of selector when using the `find_all()` method, not just `class`.  For example, imagine you have the following `<div>` tag:\n",
    "    `<div class='main-container red-blue-green' role='front-unit' aria-select='left-below'>Some content here</div>`\n",
    "    \n",
    "   This means that when you use `scraper.find_all('div')`, you can pass in arguments like `scraper.find_all('div', {'role': 'front-unit'})` or anything else that allows you to isolate that particular tag.\n",
    " - When specifying selectors like `{'class': 'dkght__384Ko'}`, sometimes less is more.  If you include multiple selectors, you are saying return a tag with **any one of these** distinctions, not all of them.  So if your results are large, try different combinations of selectors to get the smallest results possible.\n",
    " - If you begin dealing with values that are unreliably entered, you should use the 'outside in' technique where you grab a parent container that holds the element and find a way to check to see if a particular value is there by scraping it further.  The best way to do this is to try and find a unique container for every single restaurant.  This means that you will have a reliable parent element for every single restaurant, and within *each of these* you can search for `<p>`, `<a>`, `<div>`, and `<span>` tags and apply further logic.\n",
    " - When you get results from `BeautifulSoup`, you will be given data that's denoted as either `bs4.element.Tag` or `bs4.element.ResultSet`.  They are **not the same**.  Critically, you can search a `bs4.element.Tag` for further items, but you cannot do this with a `bs4.element.ResultSet`.  \n",
    " \n",
    "   For example, let's say you grab all of the divs from a page with `scraper.find_all('div')` and save it as the variable `total_divs`.  This means `total_divs` will look somethig like this:  \n",
    "   \n",
    "   `[<div><p>Div content</p><p>Second paragraph</p></div>,`\n",
    "      `<div><p>Div content</p><p>Second paragraph</p></div>,`\n",
    "      `<div><p>Div content</p><p>Second paragraph</p></div>]`\n",
    "      \n",
    "   In this case the variable `total_divs` is a result set and there's nothing else you can do to it directly.  However, every item within `total_divs` is a tag, which means you can scrape it further.  \n",
    "   \n",
    "   So if you wanted you could write a line like:  `total_paragraphs = [div.find_all('p') for div in total_divs]`, and get the collection of paragraphs within each div.  \n",
    "   \n",
    "   If you confuse the two you'll get the following error message:  \n",
    "   \n",
    "   `AttributeError: ResultSet object has no attribute 'find_all'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?`\n",
    " - The values of the different selectors change periodically on yelp, so if your scraper all of a sudden stops working that's probably why.  Ie, if you have a command like `scraper.find_all('div', {'class': '485dk0W__container09'}` that no longer returns results, the class `485dk0W__container09` may now be `r56kW__container14` or something similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jake's Coding Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier 1: Five Columns From the First Page\n",
    "- First is me showing my work\n",
    "- Below that is my cleaned-up, all-in-one-cell code block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 1 - Showing My Work\n",
    "This is going to involve a few steps:\n",
    "- Do an http \"get\" request to essentially import a webpage's source code\n",
    "- \"Scrape\" the source code with the BeautifulSoup library, which basically means we find valuable nuggets in the HTML using tricks like looking for patterns in tags and CSS classes\n",
    "- Turn that scraped data into an actual structured Pandas DataFrame (which makes it available for cool Pandas data science stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to need three Python libraries to make this code work:\n",
    "import requests   # this library will help us make http requests, which is how we get webpages\n",
    "from bs4 import BeautifulSoup   # this library will help us parse html source code (i.e., webscraping)\n",
    "import pandas as pd   # this library will help us with data science stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So let's get to requestin'!\n",
    "\n",
    "url_to_scrape = \"https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1\"\n",
    "\n",
    "http_response = requests.get(url_to_scrape)   # this actually requests the page and stores the resulting response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for knowledge, http_response is its own weirdo object, not just a string or some other standard data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of http_response is <class 'requests.models.Response'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"The data type of http_response is {type(http_response)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got the source html from Yelp, we need to put it in some kind of usable format so we can work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we'll get the text version of the http_response\n",
    "source_code_text = http_response.text   # the .text method is part of the requests library, not BeautifulSoup\n",
    "\n",
    "# then we'll give that text source code to BeautifulSoup, which creates an object with useful scraping methods\n",
    "yelp_scrape = BeautifulSoup(source_code_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for knowledge, yelp_scrape is a bs4 object, not just a string or some other standard data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of yelp_scrape is <class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"The data type of yelp_scrape is {type(yelp_scrape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this bs4 object to parse (\"scrape\") the html cource code.\n",
    "\n",
    "Tier 1 asks us to create a dataset of five restaurant attributes from the first page. I'll choose the following:\n",
    "- Restaurant name\n",
    "- Restaurant rank\n",
    "- Restaurant price range\n",
    "- Restaurant star rating\n",
    "- Restaurant neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to structure this data as a dictionary of lists\n",
    "london_yelp_restaurants = {\n",
    "    \"Name\": [],\n",
    "    \"Rank\": [],\n",
    "    \"Price range\": [],\n",
    "    \"Star rating\": [],\n",
    "    \"Neighborhood\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for names first.  \n",
    "\n",
    "We'll start by going to the website in Chrome, opening the developer tools, and inspecting restaurant title elements.  This zooms us to the corresponding html code.  I'm looking for some sort of common pattern of HTML tags or CSS classes I can use to identify all the restaurant titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"css-166la90\" href=\"/biz/the-mayfair-chippy-london-2?osq=Restaurants\" name=\"The Mayfair Chippy\" rel=\"\" target=\"\">The Mayfair Chippy</a>,\n",
       " <a class=\"css-166la90\" href=\"/biz/dishoom-london?osq=Restaurants\" name=\"Dishoom\" rel=\"\" target=\"\">Dishoom</a>,\n",
       " <a class=\"css-166la90\" href=\"/biz/flat-iron-london-2?osq=Restaurants\" name=\"Flat Iron\" rel=\"\" target=\"\">Flat Iron</a>,\n",
       " <a class=\"css-166la90\" href=\"/biz/ffionas-restaurant-london?osq=Restaurants\" name=\"Ffiona’s Restaurant\" rel=\"\" target=\"\">Ffiona’s Restaurant</a>,\n",
       " <a class=\"css-166la90\" href=\"/biz/dishoom-london-7?osq=Restaurants\" name=\"Dishoom\" rel=\"\" target=\"\">Dishoom</a>,\n",
       " <a class=\"css-166la90\" href=\"/biz/the-breakfast-club-london-2?osq=Restaurants\" name=\"The Breakfast Club\" rel=\"\" target=\"\">The Breakfast Club</a>,\n",
       " <a class=\"css-166la90\" href=\"/biz/restaurant-gordon-ramsay-london-3?osq=Restaurants\" name=\"Restaurant Gordon Ramsay\" rel=\"\" target=\"\">Restaurant Gordon Ramsay</a>,\n",
       " <a class=\"css-166la90\" href=\"/biz/the-fat-bear-london?osq=Restaurants\" name=\"The Fat Bear\" rel=\"\" target=\"\">The Fat Bear</a>,\n",
       " <a class=\"css-166la90\" href=\"/biz/nopi-london?osq=Restaurants\" name=\"NOPI\" rel=\"\" target=\"\">NOPI</a>,\n",
       " <a class=\"css-166la90\" href=\"/biz/sketch-london-2?osq=Restaurants\" name=\"Sketch\" rel=\"\" target=\"\">Sketch</a>,\n",
       " <a class=\"pagination-link-component__09f24__H0ICg css-166la90\" href=\"https://www.yelp.com/search?find_desc=Restaurants&amp;find_loc=London%2C%20United%20Kingdom&amp;start=10\" name=\"\" rel=\"\" role=\"link\" target=\"\"><div aria-label=\"Page: 2\" class=\"undefined display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\">2</div></a>,\n",
       " <a class=\"pagination-link-component__09f24__H0ICg css-166la90\" href=\"https://www.yelp.com/search?find_desc=Restaurants&amp;find_loc=London%2C%20United%20Kingdom&amp;start=20\" name=\"\" rel=\"\" role=\"link\" target=\"\"><div aria-label=\"Page: 3\" class=\"undefined display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\">3</div></a>,\n",
       " <a class=\"pagination-link-component__09f24__H0ICg css-166la90\" href=\"https://www.yelp.com/search?find_desc=Restaurants&amp;find_loc=London%2C%20United%20Kingdom&amp;start=30\" name=\"\" rel=\"\" role=\"link\" target=\"\"><div aria-label=\"Page: 4\" class=\"undefined display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\">4</div></a>,\n",
       " <a class=\"pagination-link-component__09f24__H0ICg css-166la90\" href=\"https://www.yelp.com/search?find_desc=Restaurants&amp;find_loc=London%2C%20United%20Kingdom&amp;start=40\" name=\"\" rel=\"\" role=\"link\" target=\"\"><div aria-label=\"Page: 5\" class=\"undefined display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\">5</div></a>,\n",
       " <a class=\"pagination-link-component__09f24__H0ICg css-166la90\" href=\"https://www.yelp.com/search?find_desc=Restaurants&amp;find_loc=London%2C%20United%20Kingdom&amp;start=50\" name=\"\" rel=\"\" role=\"link\" target=\"\"><div aria-label=\"Page: 6\" class=\"undefined display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\">6</div></a>,\n",
       " <a class=\"pagination-link-component__09f24__H0ICg css-166la90\" href=\"https://www.yelp.com/search?find_desc=Restaurants&amp;find_loc=London%2C%20United%20Kingdom&amp;start=60\" name=\"\" rel=\"\" role=\"link\" target=\"\"><div aria-label=\"Page: 7\" class=\"undefined display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\">7</div></a>,\n",
       " <a class=\"pagination-link-component__09f24__H0ICg css-166la90\" href=\"https://www.yelp.com/search?find_desc=Restaurants&amp;find_loc=London%2C%20United%20Kingdom&amp;start=70\" name=\"\" rel=\"\" role=\"link\" target=\"\"><div aria-label=\"Page: 8\" class=\"undefined display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\">8</div></a>,\n",
       " <a class=\"pagination-link-component__09f24__H0ICg css-166la90\" href=\"https://www.yelp.com/search?find_desc=Restaurants&amp;find_loc=London%2C%20United%20Kingdom&amp;start=80\" name=\"\" rel=\"\" role=\"link\" target=\"\"><div aria-label=\"Page: 9\" class=\"undefined display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\">9</div></a>,\n",
       " <a class=\"next-link navigation-button__09f24__3F7Pt css-166la90\" href=\"https://www.yelp.com/search?find_desc=Restaurants&amp;find_loc=London%2C%20United%20Kingdom&amp;start=10\" name=\"\" rel=\"\" role=\"link\" target=\"\"><span aria-hidden=\"true\" class=\"icon--24-chevron-right-v2 navigation-button-icon__09f24__2FnZH css-12anxc3\"><svg class=\"icon_svg\" height=\"24\" width=\"24\"><path d=\"M9.75 17.58a1 1 0 01-.7-.28 1 1 0 010-1.42l3.8-3.88L9 8.12a1 1 0 111.41-1.42L15 11.3a1 1 0 010 1.4l-4.5 4.58a1 1 0 01-.75.3z\"></path></svg></span></a>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# found the restaurant name inside of <a> tags, with a specific-looking css class, so we'll go with that\n",
    "scraped_names = yelp_scrape.find_all('a', {'class': 'css-166la90'})  # this returns a list of tags that match the criteria\n",
    "\n",
    "# let's take a look at what we found:\n",
    "scraped_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Mayfair Chippy',\n",
       " 'Dishoom',\n",
       " 'Flat Iron',\n",
       " 'Ffiona’s Restaurant',\n",
       " 'Dishoom',\n",
       " 'The Breakfast Club',\n",
       " 'Restaurant Gordon Ramsay',\n",
       " 'The Fat Bear',\n",
       " 'NOPI',\n",
       " 'Sketch',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looks like we got the goods, but we need to narrow it down more\n",
    "# this will be easier if we just get at the text between the tags using the .text method\n",
    "scraped_names = [scraped_name.text for scraped_name in scraped_names]\n",
    "\n",
    "scraped_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Mayfair Chippy',\n",
       " 'Dishoom',\n",
       " 'Flat Iron',\n",
       " 'Ffiona’s Restaurant',\n",
       " 'Dishoom',\n",
       " 'The Breakfast Club',\n",
       " 'Restaurant Gordon Ramsay',\n",
       " 'The Fat Bear',\n",
       " 'NOPI',\n",
       " 'Sketch']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now it's clear to see how we can narrow the list\n",
    "scraped_names = [name for name in scraped_names if len(name) >1]  # this will be a problem if a restaurant has a one-character name...\n",
    "\n",
    "scraped_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of scraped names: 10\n"
     ]
    }
   ],
   "source": [
    "# let's double-check to make sure there are ten results\n",
    "print(f\"# of scraped names: {len(scraped_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': ['The Mayfair Chippy',\n",
       "  'Dishoom',\n",
       "  'Flat Iron',\n",
       "  'Ffiona’s Restaurant',\n",
       "  'Dishoom',\n",
       "  'The Breakfast Club',\n",
       "  'Restaurant Gordon Ramsay',\n",
       "  'The Fat Bear',\n",
       "  'NOPI',\n",
       "  'Sketch'],\n",
       " 'Rank': [],\n",
       " 'Price range': [],\n",
       " 'Star rating': [],\n",
       " 'Neighborhood': []}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# et voila!  The names for the first ten restaurants.  Let's put it in the dictionary!\n",
    "\n",
    "london_yelp_restaurants['Name'] = scraped_names\n",
    "\n",
    "london_yelp_restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply similar logic and scrape out the rest of the data we want.  I'm purposefully going to leave in the scratchwork so I can show my logic, but maybe I'll create a cleaned-up version at the end.\n",
    "\n",
    "Next up is ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"css-1pxmz4g\">1<!-- -->. <a class=\"css-166la90\" href=\"/biz/the-mayfair-chippy-london-2?osq=Restaurants\" name=\"The Mayfair Chippy\" rel=\"\" target=\"\">The Mayfair Chippy</a></span>,\n",
       " <span class=\"css-1pxmz4g\">2<!-- -->. <a class=\"css-166la90\" href=\"/biz/dishoom-london?osq=Restaurants\" name=\"Dishoom\" rel=\"\" target=\"\">Dishoom</a></span>,\n",
       " <span class=\"css-1pxmz4g\">3<!-- -->. <a class=\"css-166la90\" href=\"/biz/flat-iron-london-2?osq=Restaurants\" name=\"Flat Iron\" rel=\"\" target=\"\">Flat Iron</a></span>,\n",
       " <span class=\"css-1pxmz4g\">4<!-- -->. <a class=\"css-166la90\" href=\"/biz/ffionas-restaurant-london?osq=Restaurants\" name=\"Ffiona’s Restaurant\" rel=\"\" target=\"\">Ffiona’s Restaurant</a></span>,\n",
       " <span class=\"css-1pxmz4g\">5<!-- -->. <a class=\"css-166la90\" href=\"/biz/dishoom-london-7?osq=Restaurants\" name=\"Dishoom\" rel=\"\" target=\"\">Dishoom</a></span>,\n",
       " <span class=\"css-1pxmz4g\">6<!-- -->. <a class=\"css-166la90\" href=\"/biz/the-breakfast-club-london-2?osq=Restaurants\" name=\"The Breakfast Club\" rel=\"\" target=\"\">The Breakfast Club</a></span>,\n",
       " <span class=\"css-1pxmz4g\">7<!-- -->. <a class=\"css-166la90\" href=\"/biz/restaurant-gordon-ramsay-london-3?osq=Restaurants\" name=\"Restaurant Gordon Ramsay\" rel=\"\" target=\"\">Restaurant Gordon Ramsay</a></span>,\n",
       " <span class=\"css-1pxmz4g\">8<!-- -->. <a class=\"css-166la90\" href=\"/biz/the-fat-bear-london?osq=Restaurants\" name=\"The Fat Bear\" rel=\"\" target=\"\">The Fat Bear</a></span>,\n",
       " <span class=\"css-1pxmz4g\">9<!-- -->. <a class=\"css-166la90\" href=\"/biz/nopi-london?osq=Restaurants\" name=\"NOPI\" rel=\"\" target=\"\">NOPI</a></span>,\n",
       " <span class=\"css-1pxmz4g\">10<!-- -->. <a class=\"css-166la90\" href=\"/biz/sketch-london-2?osq=Restaurants\" name=\"Sketch\" rel=\"\" target=\"\">Sketch</a></span>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_ranks = yelp_scrape.find_all('span', {'class': 'css-1pxmz4g'})\n",
    "\n",
    "scraped_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_ranks = [rank.text for rank in scraped_ranks]  # get the text between the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.\\xa0The Mayfair Chippy',\n",
       " '2.\\xa0Dishoom',\n",
       " '3.\\xa0Flat Iron',\n",
       " '4.\\xa0Ffiona’s Restaurant',\n",
       " '5.\\xa0Dishoom',\n",
       " '6.\\xa0The Breakfast Club',\n",
       " '7.\\xa0Restaurant Gordon Ramsay',\n",
       " '8.\\xa0The Fat Bear',\n",
       " '9.\\xa0NOPI',\n",
       " '10.\\xa0Sketch']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_ranks  # see what we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now parse out the number\n",
    "scraped_ranks = [rank[0:rank.index('.')] for rank in scraped_ranks]  # grab the first characters before the \".\"\n",
    "\n",
    "scraped_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's assume we want integers, so convert\n",
    "scraped_ranks = [int(rank) for rank in scraped_ranks]\n",
    "\n",
    "scraped_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': ['The Mayfair Chippy',\n",
       "  'Dishoom',\n",
       "  'Flat Iron',\n",
       "  'Ffiona’s Restaurant',\n",
       "  'Dishoom',\n",
       "  'The Breakfast Club',\n",
       "  'Restaurant Gordon Ramsay',\n",
       "  'The Fat Bear',\n",
       "  'NOPI',\n",
       "  'Sketch'],\n",
       " 'Rank': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " 'Price range': [],\n",
       " 'Star rating': [],\n",
       " 'Neighborhood': []}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have what we want!  Now let's add it to the dictionary\n",
    "\n",
    "london_yelp_restaurants['Rank'] = scraped_ranks\n",
    "\n",
    "london_yelp_restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do price ranges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"priceRange__09f24__2O6le css-xtpg8e\">££</span>,\n",
       " <span class=\"priceRange__09f24__2O6le css-xtpg8e\">££</span>,\n",
       " <span class=\"priceRange__09f24__2O6le css-xtpg8e\">££</span>,\n",
       " <span class=\"priceRange__09f24__2O6le css-xtpg8e\">££</span>,\n",
       " <span class=\"priceRange__09f24__2O6le css-xtpg8e\">££</span>,\n",
       " <span class=\"priceRange__09f24__2O6le css-xtpg8e\">££</span>,\n",
       " <span class=\"priceRange__09f24__2O6le css-xtpg8e\">££££</span>,\n",
       " <span class=\"priceRange__09f24__2O6le css-xtpg8e\">££</span>,\n",
       " <span class=\"priceRange__09f24__2O6le css-xtpg8e\">£££</span>,\n",
       " <span class=\"priceRange__09f24__2O6le css-xtpg8e\">£££</span>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_price_ranges = yelp_scrape.find_all('span', {'class': 'priceRange__09f24__2O6le css-xtpg8e'})\n",
    "\n",
    "scraped_price_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['££', '££', '££', '££', '££', '££', '££££', '££', '£££', '£££']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_price_ranges = [price.text for price in scraped_price_ranges]\n",
    "\n",
    "scraped_price_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': ['The Mayfair Chippy',\n",
       "  'Dishoom',\n",
       "  'Flat Iron',\n",
       "  'Ffiona’s Restaurant',\n",
       "  'Dishoom',\n",
       "  'The Breakfast Club',\n",
       "  'Restaurant Gordon Ramsay',\n",
       "  'The Fat Bear',\n",
       "  'NOPI',\n",
       "  'Sketch'],\n",
       " 'Rank': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " 'Price range': ['££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££££',\n",
       "  '££',\n",
       "  '£££',\n",
       "  '£££'],\n",
       " 'Star rating': [],\n",
       " 'Neighborhood': []}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have what we want!  Now let's add it to the dictionary\n",
    "\n",
    "london_yelp_restaurants['Price range'] = scraped_price_ranges\n",
    "\n",
    "london_yelp_restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on to star rating!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div aria-label=\"4.5 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4-half__09f24__1YrPo border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4.5 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4-half__09f24__1YrPo border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4.5 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4-half__09f24__1YrPo border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4.5 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4-half__09f24__1YrPo border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4.5 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4-half__09f24__1YrPo border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4__09f24__2YrSK border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4.5 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4-half__09f24__1YrPo border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4.5 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4-half__09f24__1YrPo border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4.5 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4-half__09f24__1YrPo border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4__09f24__2YrSK border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_star_ratings = yelp_scrape.find_all('div', {'class': 'i-stars__09f24__1T6rz'})\n",
    "\n",
    "scraped_star_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scraped_star_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_star_ratings = [rating.text for rating in scraped_star_ratings]\n",
    "\n",
    "scraped_star_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh snap!  There is no text between the tags here...can we get at this \"aria-label\" tag attribute instead of the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div aria-label=\"4.5 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4-half__09f24__1YrPo border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4.5 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4-half__09f24__1YrPo border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4.5 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4-half__09f24__1YrPo border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4.5 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4-half__09f24__1YrPo border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4.5 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4-half__09f24__1YrPo border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4__09f24__2YrSK border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4.5 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4-half__09f24__1YrPo border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4.5 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4-half__09f24__1YrPo border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4.5 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4-half__09f24__1YrPo border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>,\n",
       " <div aria-label=\"4 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4__09f24__2YrSK border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_star_ratings = yelp_scrape.find_all('div', {'class': 'i-stars__09f24__1T6rz'})\n",
    "\n",
    "scraped_star_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div aria-label=\"4.5 star rating\" class=\"i-stars__09f24__1T6rz i-stars--regular-4-half__09f24__1YrPo border-color--default__09f24__1eOdn overflow--hidden__09f24__3z7CX\" role=\"img\"><img alt=\"\" class=\"offscreen__09f24__1VFco\" height=\"560\" src=\"https://s3-media0.fl.yelpcdn.com/assets/public/stars_v2.yji-52d3d7a328db670d4402843cbddeed89.png\" width=\"132\"/></div>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_star_ratings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': ['i-stars__09f24__1T6rz',\n",
       "  'i-stars--regular-4-half__09f24__1YrPo',\n",
       "  'border-color--default__09f24__1eOdn',\n",
       "  'overflow--hidden__09f24__3z7CX'],\n",
       " 'aria-label': '4.5 star rating',\n",
       " 'role': 'img'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_star_ratings[0].attrs   # we can get the tag attributes by using the .attrs method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.5 star rating'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# that returned a dictionary, so we just need to call the key to get its corresponding value\n",
    "scraped_star_ratings[0].attrs['aria-label']  # this gives us the value for the aria-label attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.5 star rating',\n",
       " '4.5 star rating',\n",
       " '4.5 star rating',\n",
       " '4.5 star rating',\n",
       " '4.5 star rating',\n",
       " '4 star rating',\n",
       " '4.5 star rating',\n",
       " '4.5 star rating',\n",
       " '4.5 star rating',\n",
       " '4 star rating']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now that we've learned that, we can iterate through the list!\n",
    "\n",
    "scraped_star_ratings = [rating.attrs['aria-label'] for rating in scraped_star_ratings]\n",
    "\n",
    "scraped_star_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.5, 4.5, 4.5, 4.5, 4.5, 4.0, 4.5, 4.5, 4.5, 4.0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sweet!  now let's get rid of the \"star rating\" suffix and convert to a float\n",
    "scraped_star_ratings = [float(rating[0:len(rating) - len(\" star rating\")]) for rating in scraped_star_ratings]\n",
    "\n",
    "scraped_star_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scraped_star_ratings)  # double-check we still have 10 items and they match the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': ['The Mayfair Chippy',\n",
       "  'Dishoom',\n",
       "  'Flat Iron',\n",
       "  'Ffiona’s Restaurant',\n",
       "  'Dishoom',\n",
       "  'The Breakfast Club',\n",
       "  'Restaurant Gordon Ramsay',\n",
       "  'The Fat Bear',\n",
       "  'NOPI',\n",
       "  'Sketch'],\n",
       " 'Rank': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " 'Price range': ['££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££££',\n",
       "  '££',\n",
       "  '£££',\n",
       "  '£££'],\n",
       " 'Star rating': [4.5, 4.5, 4.5, 4.5, 4.5, 4.0, 4.5, 4.5, 4.5, 4.0],\n",
       " 'Neighborhood': []}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# awesome!  now we put it in the dictionary\n",
    "\n",
    "london_yelp_restaurants['Star rating'] = scraped_star_ratings\n",
    "\n",
    "london_yelp_restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick DataFrame check to make sure we're headed in the right direction.  Probably should have done this for the first scraped list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name\n",
      "Rank\n",
      "Price range\n",
      "Star rating\n",
      "Neighborhood\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Name': ['The Mayfair Chippy',\n",
       "  'Dishoom',\n",
       "  'Flat Iron',\n",
       "  'Ffiona’s Restaurant',\n",
       "  'Dishoom',\n",
       "  'The Breakfast Club',\n",
       "  'Restaurant Gordon Ramsay',\n",
       "  'The Fat Bear',\n",
       "  'NOPI',\n",
       "  'Sketch'],\n",
       " 'Rank': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " 'Price range': ['££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££££',\n",
       "  '££',\n",
       "  '£££',\n",
       "  '£££'],\n",
       " 'Star rating': [4.5, 4.5, 4.5, 4.5, 4.5, 4.0, 4.5, 4.5, 4.5, 4.0]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turns out that DataFrame requires all lists to be equal lengths, so we need to kick out the neighborhood for now\n",
    "test_dictionary = {}\n",
    "\n",
    "for element in london_yelp_restaurants:   # element here represents the key (a string), not the key+value pair\n",
    "    print(element)\n",
    "    if len(london_yelp_restaurants[element]) > 0:   # if the value's length is > 0\n",
    "        test_dictionary[element] = london_yelp_restaurants[element]   # assign they key and its corresponding value\n",
    "        \n",
    "test_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Price range</th>\n",
       "      <th>Star rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Mayfair Chippy</td>\n",
       "      <td>1</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>2</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flat Iron</td>\n",
       "      <td>3</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ffiona’s Restaurant</td>\n",
       "      <td>4</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>5</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Breakfast Club</td>\n",
       "      <td>6</td>\n",
       "      <td>££</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Restaurant Gordon Ramsay</td>\n",
       "      <td>7</td>\n",
       "      <td>££££</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Fat Bear</td>\n",
       "      <td>8</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NOPI</td>\n",
       "      <td>9</td>\n",
       "      <td>£££</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sketch</td>\n",
       "      <td>10</td>\n",
       "      <td>£££</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name  Rank Price range  Star rating\n",
       "0        The Mayfair Chippy     1          ££          4.5\n",
       "1                   Dishoom     2          ££          4.5\n",
       "2                 Flat Iron     3          ££          4.5\n",
       "3       Ffiona’s Restaurant     4          ££          4.5\n",
       "4                   Dishoom     5          ££          4.5\n",
       "5        The Breakfast Club     6          ££          4.0\n",
       "6  Restaurant Gordon Ramsay     7        ££££          4.5\n",
       "7              The Fat Bear     8          ££          4.5\n",
       "8                      NOPI     9         £££          4.5\n",
       "9                    Sketch    10         £££          4.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In retrospect, to avoid creating this type of test dictionary, it's probably best practice not to create an empty dictionary at the beginning, but rather just append dictionary key+value pairs as you go, so you can just call DataFrame whenever.\n",
    "\n",
    "In any case, a spot-check of the dataframe confirms that it matches the yelp site itself, so we're on track!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to grab our last item, the neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"css-8jxw1i\">020 7741 2233</p>,\n",
       " <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">14 North Audley Street</span></p>,\n",
       " <p class=\"css-8jxw1i\">Mayfair</p>,\n",
       " <p class=\"css-8jxw1i\">020 7420 9320</p>,\n",
       " <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">12 Upper Saint Martin's Lane</span></p>,\n",
       " <p class=\"css-8jxw1i\">Covent Garden</p>,\n",
       " <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">17 Beak Street</span></p>,\n",
       " <p class=\"css-8jxw1i\">Soho</p>,\n",
       " <p class=\"css-8jxw1i\">020 7937 4152</p>,\n",
       " <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">51 Kensington Church Street</span></p>,\n",
       " <p class=\"css-8jxw1i\">Kensington</p>,\n",
       " <p class=\"css-8jxw1i\">020 7420 9322</p>,\n",
       " <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">22 Kingly Street</span></p>,\n",
       " <p class=\"css-8jxw1i\">Soho</p>,\n",
       " <p class=\"css-8jxw1i\">020 7434 2571</p>,\n",
       " <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">33 D'Arblay Street</span></p>,\n",
       " <p class=\"css-8jxw1i\">Soho</p>,\n",
       " <p class=\"css-8jxw1i\">020 7352 4441</p>,\n",
       " <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">68 Royal Hospital Road</span></p>,\n",
       " <p class=\"css-8jxw1i\">Chelsea</p>,\n",
       " <p class=\"css-8jxw1i\">020 7236 2498</p>,\n",
       " <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">61 Carter Lane</span></p>,\n",
       " <p class=\"css-8jxw1i\">Blackfriars</p>,\n",
       " <p class=\"css-8jxw1i\">020 7494 9584</p>,\n",
       " <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">21-22 Warwick Street</span></p>,\n",
       " <p class=\"css-8jxw1i\">Soho</p>,\n",
       " <p class=\"css-8jxw1i\">020 7659 4500</p>,\n",
       " <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">9 Conduit Street</span></p>,\n",
       " <p class=\"css-8jxw1i\">Mayfair</p>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_neighborhoods = yelp_scrape.find_all('p', {'class': 'css-8jxw1i'})\n",
    "\n",
    "scraped_neighborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, crud.  How do we reliably find the neighborhoods in this list?\n",
    "\n",
    "It's not quite every third element in the list, because one of them is missing a phone number.  And what would happen if neighborhood data is blank/null/non-existent?\n",
    "\n",
    "We need a more structural way of finding it, and assigning a `None` value if we can't find it.\n",
    "\n",
    "This is where the outside-in approach comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7741 2233</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">14 North Audley Street</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Mayfair</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7420 9320</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">12 Upper Saint Martin's Lane</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Covent Garden</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">17 Beak Street</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Soho</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7937 4152</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">51 Kensington Church Street</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Kensington</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7420 9322</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">22 Kingly Street</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Soho</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7434 2571</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">33 D'Arblay Street</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Soho</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7352 4441</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">68 Royal Hospital Road</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Chelsea</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7236 2498</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">61 Carter Lane</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Blackfriars</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7494 9584</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">21-22 Warwick Street</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Soho</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7659 4500</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">9 Conduit Street</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Mayfair</p></div></div></div></div></div>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Chrome's inspector, I found a div tag that contains the address, phone number, neighborhood, etc.\n",
    "scraped_neighborhood_containers = yelp_scrape.find_all('div', {'class': 'container__09f24__1fWZl'})\n",
    "\n",
    "scraped_neighborhood_containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seems promising; let's confirm there are ten\n",
    "len(scraped_neighborhood_containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7741 2233</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">14 North Audley Street</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Mayfair</p></div></div></div></div></div>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cool!  let's look at just one of them\n",
    "scraped_neighborhood_containers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"css-8jxw1i\">020 7420 9322</p>,\n",
       " <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">22 Kingly Street</span></p>,\n",
       " <p class=\"css-8jxw1i\">Soho</p>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it looks like our neighbrohood is in a <p> tag, so let's find_all within the a container\n",
    "scraped_neighborhood_containers[4].find_all('p')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use that structure to pick the last element.\n",
    "\n",
    "Side note: this isn't a 100%-foolproof method, because it'll pick up addresses or phone numbers if the neighborhood is missing.  We could get closer to 100% (but not quite there) by checking to see if the string begins with a number, which will only mislead us if the neighborhood starts with a number or the address doesn't.  This also won't cover the case where all three are missing...but for now, let's just get the \"good enough\" solution for now and worry about those fringe cases later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So where were we?  Doing a more structural outside-in approach for finding neighborhood data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7741 2233</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">14 North Audley Street</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Mayfair</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7420 9320</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">12 Upper Saint Martin's Lane</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Covent Garden</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">17 Beak Street</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Soho</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7937 4152</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">51 Kensington Church Street</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Kensington</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7420 9322</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">22 Kingly Street</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Soho</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7434 2571</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">33 D'Arblay Street</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Soho</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7352 4441</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">68 Royal Hospital Road</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Chelsea</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7236 2498</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">61 Carter Lane</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Blackfriars</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7494 9584</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">21-22 Warwick Street</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Soho</p></div></div></div></div></div>,\n",
       " <div class=\"container__09f24__1fWZl padding-l2__09f24__2MHQ3 border-color--default__09f24__1eOdn text-align--right__09f24__2OpQD\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">020 7659 4500</p></div></div></div><address class=\"\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">9 Conduit Street</span></p></div></div></div></address><div class=\"margin-b1__09f24__1647o border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><div class=\"display--inline-block__09f24__3L1EB border-color--default__09f24__1eOdn\"><div class=\"border-color--default__09f24__1eOdn\"><p class=\"css-8jxw1i\">Mayfair</p></div></div></div></div></div>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the list of div tags that contain the address, phone number, neighborhood, etc.\n",
    "scraped_neighborhood_containers = yelp_scrape.find_all('div', {'class': 'container__09f24__1fWZl'})\n",
    "\n",
    "scraped_neighborhood_containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<p class=\"css-8jxw1i\">020 7741 2233</p>,\n",
       "  <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">14 North Audley Street</span></p>,\n",
       "  <p class=\"css-8jxw1i\">Mayfair</p>],\n",
       " [<p class=\"css-8jxw1i\">020 7420 9320</p>,\n",
       "  <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">12 Upper Saint Martin's Lane</span></p>,\n",
       "  <p class=\"css-8jxw1i\">Covent Garden</p>],\n",
       " [<p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">17 Beak Street</span></p>,\n",
       "  <p class=\"css-8jxw1i\">Soho</p>],\n",
       " [<p class=\"css-8jxw1i\">020 7937 4152</p>,\n",
       "  <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">51 Kensington Church Street</span></p>,\n",
       "  <p class=\"css-8jxw1i\">Kensington</p>],\n",
       " [<p class=\"css-8jxw1i\">020 7420 9322</p>,\n",
       "  <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">22 Kingly Street</span></p>,\n",
       "  <p class=\"css-8jxw1i\">Soho</p>],\n",
       " [<p class=\"css-8jxw1i\">020 7434 2571</p>,\n",
       "  <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">33 D'Arblay Street</span></p>,\n",
       "  <p class=\"css-8jxw1i\">Soho</p>],\n",
       " [<p class=\"css-8jxw1i\">020 7352 4441</p>,\n",
       "  <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">68 Royal Hospital Road</span></p>,\n",
       "  <p class=\"css-8jxw1i\">Chelsea</p>],\n",
       " [<p class=\"css-8jxw1i\">020 7236 2498</p>,\n",
       "  <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">61 Carter Lane</span></p>,\n",
       "  <p class=\"css-8jxw1i\">Blackfriars</p>],\n",
       " [<p class=\"css-8jxw1i\">020 7494 9584</p>,\n",
       "  <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">21-22 Warwick Street</span></p>,\n",
       "  <p class=\"css-8jxw1i\">Soho</p>],\n",
       " [<p class=\"css-8jxw1i\">020 7659 4500</p>,\n",
       "  <p class=\"css-8jxw1i\"><span class=\"raw__09f24__3Obuy\">9 Conduit Street</span></p>,\n",
       "  <p class=\"css-8jxw1i\">Mayfair</p>]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's just get the <p> tags within each container\n",
    "scraped_neighborhoods = [container.find_all('p') for container in scraped_neighborhood_containers]\n",
    "\n",
    "scraped_neighborhoods  # this is a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['020 7741 2233', '14 North Audley Street', 'Mayfair'],\n",
       " ['020 7420 9320', \"12 Upper Saint Martin's Lane\", 'Covent Garden'],\n",
       " ['17 Beak Street', 'Soho'],\n",
       " ['020 7937 4152', '51 Kensington Church Street', 'Kensington'],\n",
       " ['020 7420 9322', '22 Kingly Street', 'Soho'],\n",
       " ['020 7434 2571', \"33 D'Arblay Street\", 'Soho'],\n",
       " ['020 7352 4441', '68 Royal Hospital Road', 'Chelsea'],\n",
       " ['020 7236 2498', '61 Carter Lane', 'Blackfriars'],\n",
       " ['020 7494 9584', '21-22 Warwick Street', 'Soho'],\n",
       " ['020 7659 4500', '9 Conduit Street', 'Mayfair']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's simplify what we're looking at by getting text of each tag\n",
    "\n",
    "for collection in scraped_neighborhoods:\n",
    "    for tag in range(0,len(collection)):\n",
    "        collection[tag] = collection[tag].text\n",
    "\n",
    "scraped_neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mayfair',\n",
       " 'Covent Garden',\n",
       " 'Soho',\n",
       " 'Kensington',\n",
       " 'Soho',\n",
       " 'Soho',\n",
       " 'Chelsea',\n",
       " 'Blackfriars',\n",
       " 'Soho',\n",
       " 'Mayfair']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's get the last element of each list\n",
    "\n",
    "scraped_neighborhoods = [location[len(location)-1] for location in scraped_neighborhoods]\n",
    "\n",
    "scraped_neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scraped_neighborhoods)  # double-check we still have 10 items and they match the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': ['The Mayfair Chippy',\n",
       "  'Dishoom',\n",
       "  'Flat Iron',\n",
       "  'Ffiona’s Restaurant',\n",
       "  'Dishoom',\n",
       "  'The Breakfast Club',\n",
       "  'Restaurant Gordon Ramsay',\n",
       "  'The Fat Bear',\n",
       "  'NOPI',\n",
       "  'Sketch'],\n",
       " 'Rank': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " 'Price range': ['££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££££',\n",
       "  '££',\n",
       "  '£££',\n",
       "  '£££'],\n",
       " 'Star rating': [4.5, 4.5, 4.5, 4.5, 4.5, 4.0, 4.5, 4.5, 4.5, 4.0],\n",
       " 'Neighborhood': ['Mayfair',\n",
       "  'Covent Garden',\n",
       "  'Soho',\n",
       "  'Kensington',\n",
       "  'Soho',\n",
       "  'Soho',\n",
       "  'Chelsea',\n",
       "  'Blackfriars',\n",
       "  'Soho',\n",
       "  'Mayfair']}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# awesome!  now we put it in the dictionary\n",
    "\n",
    "london_yelp_restaurants['Neighborhood'] = scraped_neighborhoods\n",
    "\n",
    "london_yelp_restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Price range</th>\n",
       "      <th>Star rating</th>\n",
       "      <th>Neighborhood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Mayfair Chippy</td>\n",
       "      <td>1</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Mayfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>2</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Covent Garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flat Iron</td>\n",
       "      <td>3</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ffiona’s Restaurant</td>\n",
       "      <td>4</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Kensington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>5</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Breakfast Club</td>\n",
       "      <td>6</td>\n",
       "      <td>££</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Restaurant Gordon Ramsay</td>\n",
       "      <td>7</td>\n",
       "      <td>££££</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Chelsea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Fat Bear</td>\n",
       "      <td>8</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Blackfriars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NOPI</td>\n",
       "      <td>9</td>\n",
       "      <td>£££</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sketch</td>\n",
       "      <td>10</td>\n",
       "      <td>£££</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Mayfair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name  Rank Price range  Star rating   Neighborhood\n",
       "0        The Mayfair Chippy     1          ££          4.5        Mayfair\n",
       "1                   Dishoom     2          ££          4.5  Covent Garden\n",
       "2                 Flat Iron     3          ££          4.5           Soho\n",
       "3       Ffiona’s Restaurant     4          ££          4.5     Kensington\n",
       "4                   Dishoom     5          ££          4.5           Soho\n",
       "5        The Breakfast Club     6          ££          4.0           Soho\n",
       "6  Restaurant Gordon Ramsay     7        ££££          4.5        Chelsea\n",
       "7              The Fat Bear     8          ££          4.5    Blackfriars\n",
       "8                      NOPI     9         £££          4.5           Soho\n",
       "9                    Sketch    10         £££          4.0        Mayfair"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's update that DataFrame!\n",
    "\n",
    "pd.DataFrame(london_yelp_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the product we were looking for, so let's refactor all of the above into a nice, clean block!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 1 - Cleaned-Up Code Answer\n",
    "Below is my cleaned-up code that executes everything in a single cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Price range</th>\n",
       "      <th>Star rating</th>\n",
       "      <th>Neighborhood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Mayfair Chippy</td>\n",
       "      <td>1</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Mayfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>2</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Covent Garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flat Iron</td>\n",
       "      <td>3</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ffiona’s Restaurant</td>\n",
       "      <td>4</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Kensington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>5</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Breakfast Club</td>\n",
       "      <td>6</td>\n",
       "      <td>££</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Restaurant Gordon Ramsay</td>\n",
       "      <td>7</td>\n",
       "      <td>££££</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Chelsea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Fat Bear</td>\n",
       "      <td>8</td>\n",
       "      <td>££</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Blackfriars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NOPI</td>\n",
       "      <td>9</td>\n",
       "      <td>£££</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sketch</td>\n",
       "      <td>10</td>\n",
       "      <td>£££</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Mayfair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name  Rank Price range  Star rating   Neighborhood\n",
       "0        The Mayfair Chippy     1          ££          4.5        Mayfair\n",
       "1                   Dishoom     2          ££          4.5  Covent Garden\n",
       "2                 Flat Iron     3          ££          4.5           Soho\n",
       "3       Ffiona’s Restaurant     4          ££          4.5     Kensington\n",
       "4                   Dishoom     5          ££          4.5           Soho\n",
       "5        The Breakfast Club     6          ££          4.0           Soho\n",
       "6  Restaurant Gordon Ramsay     7        ££££          4.5        Chelsea\n",
       "7              The Fat Bear     8          ££          4.5    Blackfriars\n",
       "8                      NOPI     9         £££          4.5           Soho\n",
       "9                    Sketch    10         £££          4.0        Mayfair"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're going to need three Python libraries to make this code work:\n",
    "\n",
    "import requests   # this library will help us make http requests, which is how we get webpages\n",
    "from bs4 import BeautifulSoup   # this library will help us parse html source code (i.e., webscraping)\n",
    "import pandas as pd   # this library will help us with data science stuff\n",
    "\n",
    "\n",
    "\n",
    "# So let's get to requestin'!\n",
    "url_to_scrape = \"https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1\"\n",
    "\n",
    "http_response = requests.get(url_to_scrape)   # this actually requests the page and stores the resulting response\n",
    "\n",
    "# get the text version of the http_response\n",
    "source_code_text = http_response.text   # the .text method is part of the requests library, not BeautifulSoup\n",
    "\n",
    "# then we'll give that text source code to BeautifulSoup, which creates an object with useful scraping methods\n",
    "yelp_scrape = BeautifulSoup(source_code_text)\n",
    "\n",
    "\n",
    "\n",
    "# we're going to structure this data as a dictionary of lists\n",
    "london_yelp_restaurants = {}\n",
    "\n",
    "\n",
    "\n",
    "# let's scrape for names:\n",
    "scraped_names = yelp_scrape.find_all('a', {'class': 'css-166la90'})  # this returns a list of tags that match the criteria\n",
    "\n",
    "# get the text\n",
    "scraped_names = [scraped_name.text for scraped_name in scraped_names]\n",
    "\n",
    "# clean it up\n",
    "scraped_names = [name for name in scraped_names if len(name) >1]  # this will be a problem if a restaurant has a one-character name...\n",
    "\n",
    "# add it to the dictionary\n",
    "london_yelp_restaurants['Name'] = scraped_names\n",
    "\n",
    "\n",
    "\n",
    "# now let's scrape for ranks:\n",
    "scraped_ranks = yelp_scrape.find_all('span', {'class': 'css-1pxmz4g'})\n",
    "\n",
    "# get the text\n",
    "scraped_ranks = [rank.text for rank in scraped_ranks]  # get the text between the tags\n",
    "\n",
    "# now parse out the number\n",
    "scraped_ranks = [rank[0:rank.index('.')] for rank in scraped_ranks]  # grab the first characters before the \".\"\n",
    "\n",
    "# let's assume we want integers, so convert\n",
    "scraped_ranks = [int(rank) for rank in scraped_ranks]\n",
    "\n",
    "# add it to the dictionary\n",
    "london_yelp_restaurants['Rank'] = scraped_ranks\n",
    "\n",
    "\n",
    "\n",
    "# now let's scrape for price ranges:\n",
    "scraped_price_ranges = yelp_scrape.find_all('span', {'class': 'priceRange__09f24__2O6le css-xtpg8e'})\n",
    "\n",
    "# get the text\n",
    "scraped_price_ranges = [price.text for price in scraped_price_ranges]\n",
    "\n",
    "# add it to the dictionary\n",
    "london_yelp_restaurants['Price range'] = scraped_price_ranges\n",
    "\n",
    "\n",
    "\n",
    "# now let's scrape for star rating:\n",
    "scraped_star_ratings = yelp_scrape.find_all('div', {'class': 'i-stars__09f24__1T6rz'})\n",
    "\n",
    "# narrow it down\n",
    "scraped_star_ratings = [rating.attrs['aria-label'] for rating in scraped_star_ratings]\n",
    "\n",
    "# clean it up by getting rid of the \"star rating\" suffix and converting to a float\n",
    "scraped_star_ratings = [float(rating[0:len(rating) - len(\" star rating\")]) for rating in scraped_star_ratings]\n",
    "\n",
    "# add it to the dictionary\n",
    "london_yelp_restaurants['Star rating'] = scraped_star_ratings\n",
    "\n",
    "\n",
    "\n",
    "# now let's scrape for neighborhood:\n",
    "scraped_neighborhood_containers = yelp_scrape.find_all('div', {'class': 'container__09f24__1fWZl'})\n",
    "\n",
    "# now let's just get the <p> tags within each container\n",
    "scraped_neighborhoods = [container.find_all('p') for container in scraped_neighborhood_containers]\n",
    "\n",
    "# let's simplify what we're looking at by getting text of each tag\n",
    "for collection in scraped_neighborhoods:\n",
    "    for tag in range(0,len(collection)):\n",
    "        collection[tag] = collection[tag].text   # is there a more efficienty way to do this?\n",
    "        \n",
    "# now let's get the last element of each list\n",
    "scraped_neighborhoods = [location[len(location)-1] for location in scraped_neighborhoods]\n",
    "\n",
    "# add it to the dictionary\n",
    "london_yelp_restaurants['Neighborhood'] = scraped_neighborhoods\n",
    "\n",
    "\n",
    "\n",
    "# now put it in a DataFrame!\n",
    "pd.DataFrame(london_yelp_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiers 2 and 3:  100 Row Dataset With At Least 3 and 5 Columns, Respectively\n",
    "Since Tier 3 builds off of Tier 2, I'm jumping right to Tier 3 so I can kill two birds with one stone.\n",
    "- First is me showing my work\n",
    "- Below that is my cleaned-up, all-in-one-cell code block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiers 2 and 3 - Showing My Work\n",
    "This is going to involve a few steps:\n",
    "- Figure out how to modify the Yelp page URL to load the next ten results (or load more than ten)\n",
    "- Do webscraping using the same methods as Tier 1\n",
    " - Figure out how to handle missing values and replace them with `None` so we feed the same lenth lists into the DataFrame\n",
    "- Generate the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Yelp page won't let me load more than ten results, so we'll have to scrape ten results at a time.\n",
    "\n",
    "To do this, we'll load the first page, scrape, load the next page, scrape, load the next page, etc.\n",
    "\n",
    "To get the next page, we'll have to add a `&start=10` parameter to the end of the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=0',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=10',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=20',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=30',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=40',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=50',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=60',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=70',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=80',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=90']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = \"https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1\"\n",
    "\n",
    "# generate a list like [0, 10, 20, ..., 90] to get the first ten pages (first 100 results)\n",
    "page_start_values = [i * 10 for i in range(0,10)] \n",
    "\n",
    "# create a list of URLs to scrape\n",
    "urls_to_scrape = [f\"{base_url}&start={value}\" for value in page_start_values]   # append a suffix to the base url\n",
    "\n",
    "urls_to_scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take the cleaned-up code from Tier 1 and put it to good use!\n",
    "\n",
    "We'll loop through the code ten times, each time scraping a different URL from the list we just created.\n",
    "\n",
    "This will be easier to read if we define the scraper as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to need three Python libraries to make this code work:\n",
    "\n",
    "import requests   # this library will help us make http requests, which is how we get webpages\n",
    "from bs4 import BeautifulSoup   # this library will help us parse html source code (i.e., webscraping)\n",
    "import pandas as pd   # this library will help us with data science stuff\n",
    "\n",
    "\n",
    "\n",
    "# make a function similar to the code in Tier 1, but have it return a dictionary (and don't create a DataFrame)\n",
    "def scrape_a_page(site_to_scrape):\n",
    "    # So let's get to requestin'!\n",
    "    http_response = requests.get(site_to_scrape)   # this actually requests the page and stores the resulting response\n",
    "\n",
    "    # get the text version of the http_response\n",
    "    source_code_text = http_response.text   # the .text method is part of the requests library, not BeautifulSoup\n",
    "\n",
    "    # then we'll give that text source code to BeautifulSoup, which creates an object with useful scraping methods\n",
    "    yelp_scrape = BeautifulSoup(source_code_text)\n",
    "\n",
    "\n",
    "\n",
    "    # we're going to structure this data as a dictionary of lists\n",
    "    london_yelp_restaurants = {}\n",
    "\n",
    "\n",
    "\n",
    "    # let's scrape for names:\n",
    "    scraped_names = yelp_scrape.find_all('a', {'class': 'css-166la90'})  # this returns a list of tags that match the criteria\n",
    "\n",
    "    # get the text\n",
    "    scraped_names = [scraped_name.text for scraped_name in scraped_names]\n",
    "\n",
    "    # clean it up\n",
    "    scraped_names = [name for name in scraped_names if len(name) >1]  # this will be a problem if a restaurant has a one-character name...\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Name'] = scraped_names\n",
    "\n",
    "\n",
    "\n",
    "    # now let's scrape for ranks:\n",
    "    scraped_ranks = yelp_scrape.find_all('span', {'class': 'css-1pxmz4g'})\n",
    "\n",
    "    # get the text\n",
    "    scraped_ranks = [rank.text for rank in scraped_ranks]  # get the text between the tags\n",
    "\n",
    "    # now parse out the number\n",
    "    scraped_ranks = [rank[0:rank.index('.')] for rank in scraped_ranks]  # grab the first characters before the \".\"\n",
    "\n",
    "    # let's assume we want integers, so convert\n",
    "    scraped_ranks = [int(rank) for rank in scraped_ranks]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Rank'] = scraped_ranks\n",
    "\n",
    "\n",
    "\n",
    "    # now let's scrape for price ranges:\n",
    "    scraped_price_ranges = yelp_scrape.find_all('span', {'class': 'priceRange__09f24__2O6le css-xtpg8e'})\n",
    "\n",
    "    # get the text\n",
    "    scraped_price_ranges = [price.text for price in scraped_price_ranges]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Price range'] = scraped_price_ranges\n",
    "\n",
    "\n",
    "\n",
    "    # now let's scrape for star rating:\n",
    "    scraped_star_ratings = yelp_scrape.find_all('div', {'class': 'i-stars__09f24__1T6rz'})\n",
    "\n",
    "    # narrow it down\n",
    "    scraped_star_ratings = [rating.attrs['aria-label'] for rating in scraped_star_ratings]\n",
    "\n",
    "    # clean it up by getting rid of the \"star rating\" suffix and converting to a float\n",
    "    scraped_star_ratings = [float(rating[0:len(rating) - len(\" star rating\")]) for rating in scraped_star_ratings]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Star rating'] = scraped_star_ratings\n",
    "\n",
    "\n",
    "\n",
    "    # now let's scrape for neighborhood:\n",
    "    scraped_neighborhood_containers = yelp_scrape.find_all('div', {'class': 'container__09f24__1fWZl'})\n",
    "\n",
    "    # now let's just get the <p> tags within each container\n",
    "    scraped_neighborhoods = [container.find_all('p') for container in scraped_neighborhood_containers]\n",
    "\n",
    "    # let's simplify what we're looking at by getting text of each tag\n",
    "    for collection in scraped_neighborhoods:\n",
    "        for tag in range(0,len(collection)):\n",
    "            collection[tag] = collection[tag].text   # is there a more efficienty way to do this?\n",
    "\n",
    "    # now let's get the last element of each list\n",
    "    scraped_neighborhoods = [location[len(location)-1] for location in scraped_neighborhoods]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Neighborhood'] = scraped_neighborhoods\n",
    "\n",
    "    return london_yelp_restaurants  # return the dictionary with the results of the single-page scrape\n",
    "\n",
    "\n",
    "\n",
    "# so let's loop through all the pages we want to scrape, and call the function\n",
    "\n",
    "# first, create a dictionary to hold the results (a dictionary of lists)\n",
    "london_yelp_restaurants = {\n",
    "    \"Name\": [],\n",
    "    \"Rank\": [],\n",
    "    \"Price range\": [],\n",
    "    \"Star rating\": [],\n",
    "    \"Neighborhood\": []\n",
    "}\n",
    "\n",
    "\n",
    "# now let's call the functions!\n",
    "for page in urls_to_scrape:  # for each site in the list of URLs to scrape\n",
    "    # call the scrape function for the page and store it in a dictionary\n",
    "    single_page_yelp_results = scrape_a_page(page)\n",
    "    \n",
    "    # now append the single-page results to the main results dictionary\n",
    "    for key in london_yelp_restaurants:\n",
    "        london_yelp_restaurants[key] += single_page_yelp_results[key]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': ['The Mayfair Chippy',\n",
       "  'Dishoom',\n",
       "  'Flat Iron',\n",
       "  'Ffiona’s Restaurant',\n",
       "  'Dishoom',\n",
       "  'The Breakfast Club',\n",
       "  'Restaurant Gordon Ramsay',\n",
       "  'The Fat Bear',\n",
       "  'NOPI',\n",
       "  'Sketch',\n",
       "  'The Golden Chippy',\n",
       "  'Honest Burgers Meard St - Soho',\n",
       "  'Padella',\n",
       "  'Mestizo',\n",
       "  'BAO - Soho',\n",
       "  'Hawksmoor Seven Dials',\n",
       "  'Lanzhou Noodle Bar',\n",
       "  'The Queens Arms',\n",
       "  'Duck & Waffle',\n",
       "  'The Palomar Restaurant',\n",
       "  'Wahaca',\n",
       "  'Barrafina',\n",
       "  'Mother Mash',\n",
       "  'Blacklock',\n",
       "  'Kennington Lane Cafe',\n",
       "  'Burger & Lobster',\n",
       "  'Homeslice Neal’s Yard',\n",
       "  'Yauatcha',\n",
       "  'Bocca Di Lupo',\n",
       "  'Abeno',\n",
       "  'Honey & Co',\n",
       "  'Kiln',\n",
       "  'Bibimbap',\n",
       "  'The Rum Kitchen',\n",
       "  'Busaba Soho',\n",
       "  'Savoir Faire',\n",
       "  'The Ledbury',\n",
       "  'Shoryu Ramen',\n",
       "  'Jinjuu',\n",
       "  'Regency Café',\n",
       "  'The Wolseley',\n",
       "  'Gordon Ramsay Street Pizza',\n",
       "  'Silk Road',\n",
       "  'Misato',\n",
       "  'San Carlo Cicchetti',\n",
       "  'Barrafina',\n",
       "  'Soho Joe',\n",
       "  'The Orangery',\n",
       "  'Nando’s',\n",
       "  'Bodean’s',\n",
       "  'The Alchemist',\n",
       "  'Circolo Popolare',\n",
       "  'Ye Olde Cheshire Cheese',\n",
       "  'The Grazing Goat',\n",
       "  'Kanada-Ya',\n",
       "  'Dominique Ansel Treehouse',\n",
       "  'Bageriet',\n",
       "  'Where the Pancakes are',\n",
       "  'Hoppers',\n",
       "  'The English Rose Cafe and Tea Shop',\n",
       "  '10',\n",
       "  'Koya Soho',\n",
       "  'Dinner by Heston Blumenthal',\n",
       "  'Burger & Lobster',\n",
       "  'Barrafina',\n",
       "  'Yalla Yalla',\n",
       "  'Piccolino',\n",
       "  'Maria Sabina',\n",
       "  'Flat Iron',\n",
       "  'The Victoria',\n",
       "  'Roti King',\n",
       "  '10',\n",
       "  '11',\n",
       "  'Tayyabs',\n",
       "  'The Ivy',\n",
       "  'Da Mario Restaurant',\n",
       "  'Wok to Walk - Oxford St',\n",
       "  'Patara',\n",
       "  'Kazu',\n",
       "  'Mr Fogg’s Tavern',\n",
       "  'Duck & Waffle Local',\n",
       "  'Naru',\n",
       "  'Absurd Bird Soho',\n",
       "  '10',\n",
       "  '11',\n",
       "  '12',\n",
       "  'Mister Lasagna',\n",
       "  'Bailey’s Fish and Chips',\n",
       "  'Seoul Bakery',\n",
       "  'Honey & Smoke',\n",
       "  'Golden Union Fish Bar',\n",
       "  'The Table',\n",
       "  'Lupita',\n",
       "  'Hawksmoor',\n",
       "  'Burger & Lobster',\n",
       "  'Feya',\n",
       "  '10',\n",
       "  '11',\n",
       "  '12',\n",
       "  '13',\n",
       "  'Fuckoffee',\n",
       "  'Nando’s',\n",
       "  'Darjeeling Express',\n",
       "  'Bill’s',\n",
       "  'ROVI',\n",
       "  'Cecconi’s Mayfair',\n",
       "  'Laksamania',\n",
       "  'The Barbary',\n",
       "  'La Porchetta Pollo Bar',\n",
       "  'Belgo Centraal',\n",
       "  '11',\n",
       "  '12',\n",
       "  '13',\n",
       "  '14'],\n",
       " 'Rank': [1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100],\n",
       " 'Price range': ['££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££££',\n",
       "  '££',\n",
       "  '£££',\n",
       "  '£££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '£££',\n",
       "  '£',\n",
       "  '££',\n",
       "  '£££',\n",
       "  '£££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '£££',\n",
       "  '£',\n",
       "  '££',\n",
       "  '££',\n",
       "  '£££',\n",
       "  '£££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '£',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££££',\n",
       "  '££',\n",
       "  '£££',\n",
       "  '£',\n",
       "  '£££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '£',\n",
       "  '£££',\n",
       "  '£££',\n",
       "  '£££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '£',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££££',\n",
       "  '£££',\n",
       "  '£££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '£',\n",
       "  '££',\n",
       "  '£££',\n",
       "  '££',\n",
       "  '£',\n",
       "  '£££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '£££',\n",
       "  '£',\n",
       "  '£',\n",
       "  '£',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '££',\n",
       "  '£££',\n",
       "  '££',\n",
       "  '£',\n",
       "  '££',\n",
       "  '££',\n",
       "  '£££',\n",
       "  '£££',\n",
       "  '£',\n",
       "  '££'],\n",
       " 'Star rating': [4.5,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  5.0,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  5.0,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  5.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  5.0,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.5,\n",
       "  4.5,\n",
       "  4.0,\n",
       "  4.0],\n",
       " 'Neighborhood': ['Mayfair',\n",
       "  'Covent Garden',\n",
       "  'Soho',\n",
       "  'Kensington',\n",
       "  'Soho',\n",
       "  'Soho',\n",
       "  'Chelsea',\n",
       "  'Blackfriars',\n",
       "  'Soho',\n",
       "  'Mayfair',\n",
       "  'Deptford',\n",
       "  'Soho',\n",
       "  'London Bridge',\n",
       "  'Euston',\n",
       "  'Soho',\n",
       "  'Covent Garden',\n",
       "  'Covent Garden',\n",
       "  'Victoria',\n",
       "  'Aldgate',\n",
       "  'Chinatown',\n",
       "  'Soho',\n",
       "  'Covent Garden',\n",
       "  'Soho',\n",
       "  'Soho',\n",
       "  'Vauxhall',\n",
       "  'Soho',\n",
       "  'Covent Garden',\n",
       "  'Soho',\n",
       "  'Soho',\n",
       "  'Bloomsbury',\n",
       "  'Fitzrovia',\n",
       "  'Soho',\n",
       "  'Bloomsbury',\n",
       "  'Soho',\n",
       "  'Soho',\n",
       "  'Bloomsbury',\n",
       "  'Notting Hill',\n",
       "  \"St James's\",\n",
       "  'Soho',\n",
       "  'Westminster',\n",
       "  'Mayfair',\n",
       "  'The City',\n",
       "  'Camberwell',\n",
       "  'Leicester Square',\n",
       "  \"St James's\",\n",
       "  'Soho',\n",
       "  'Soho',\n",
       "  'Kensington Gardens',\n",
       "  'Soho',\n",
       "  'Soho',\n",
       "  'Covent Garden',\n",
       "  'Fitzrovia',\n",
       "  'Blackfriars',\n",
       "  'Marylebone',\n",
       "  'Bloomsbury',\n",
       "  'Covent Garden',\n",
       "  'Covent Garden',\n",
       "  'Borough',\n",
       "  'Soho',\n",
       "  'Victoria',\n",
       "  'Soho',\n",
       "  'Hyde Park',\n",
       "  'Mayfair',\n",
       "  'Strand',\n",
       "  'Soho',\n",
       "  'Mayfair',\n",
       "  'Southwark',\n",
       "  'Strand',\n",
       "  'Paddington',\n",
       "  'Euston',\n",
       "  'Whitechapel',\n",
       "  'Covent Garden',\n",
       "  'Kensington',\n",
       "  'Soho',\n",
       "  'Bloomsbury',\n",
       "  'Fitzrovia',\n",
       "  'Covent Garden',\n",
       "  'Leicester Square',\n",
       "  'Covent Garden',\n",
       "  'Soho',\n",
       "  'Soho',\n",
       "  'Fulham',\n",
       "  'Bloomsbury',\n",
       "  'Fitzrovia',\n",
       "  'Soho',\n",
       "  'Southwark',\n",
       "  'Kensington',\n",
       "  'Mayfair',\n",
       "  'Belgravia',\n",
       "  'Marylebone',\n",
       "  'Borough',\n",
       "  'South Kensington',\n",
       "  'Soho',\n",
       "  'Soho',\n",
       "  'Fitzrovia',\n",
       "  'Mayfair',\n",
       "  'Fitzrovia',\n",
       "  'Covent Garden',\n",
       "  'Bloomsbury',\n",
       "  'Covent Garden']}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's see what the result is!\n",
    "london_yelp_restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the code ran without producing errors, but the results aren't right.\n",
    "\n",
    "It seems there's a problem with the restaurant names, which is honestly the one I thought would be bulletproof.  Let's run some tests to troubleshoot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(london_yelp_restaurants['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length is longer than 100, so that's a problem.  There are also numbers in the mix instead of names, which seems wrong.  If we know what they are, we can go to the specific page in the browser and check on them to see if we can find a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At index 0 (restaurant 1), the restaurant name has letters: The Mayfair Chippy\n",
      "At index 1 (restaurant 2), the restaurant name has letters: Dishoom\n",
      "At index 2 (restaurant 3), the restaurant name has letters: Flat Iron\n",
      "At index 3 (restaurant 4), the restaurant name has letters: Ffiona’s Restaurant\n",
      "At index 4 (restaurant 5), the restaurant name has letters: Dishoom\n",
      "At index 5 (restaurant 6), the restaurant name has letters: The Breakfast Club\n",
      "At index 6 (restaurant 7), the restaurant name has letters: Restaurant Gordon Ramsay\n",
      "At index 7 (restaurant 8), the restaurant name has letters: The Fat Bear\n",
      "At index 8 (restaurant 9), the restaurant name has letters: NOPI\n",
      "At index 9 (restaurant 10), the restaurant name has letters: Sketch\n",
      "At index 10 (restaurant 11), the restaurant name has letters: The Golden Chippy\n",
      "At index 11 (restaurant 12), the restaurant name has letters: Honest Burgers Meard St - Soho\n",
      "At index 12 (restaurant 13), the restaurant name has letters: Padella\n",
      "At index 13 (restaurant 14), the restaurant name has letters: Mestizo\n",
      "At index 14 (restaurant 15), the restaurant name has letters: BAO - Soho\n",
      "At index 15 (restaurant 16), the restaurant name has letters: Hawksmoor Seven Dials\n",
      "At index 16 (restaurant 17), the restaurant name has letters: Lanzhou Noodle Bar\n",
      "At index 17 (restaurant 18), the restaurant name has letters: The Queens Arms\n",
      "At index 18 (restaurant 19), the restaurant name has letters: Duck & Waffle\n",
      "At index 19 (restaurant 20), the restaurant name has letters: The Palomar Restaurant\n",
      "At index 20 (restaurant 21), the restaurant name has letters: Wahaca\n",
      "At index 21 (restaurant 22), the restaurant name has letters: Barrafina\n",
      "At index 22 (restaurant 23), the restaurant name has letters: Mother Mash\n",
      "At index 23 (restaurant 24), the restaurant name has letters: Blacklock\n",
      "At index 24 (restaurant 25), the restaurant name has letters: Kennington Lane Cafe\n",
      "At index 25 (restaurant 26), the restaurant name has letters: Burger & Lobster\n",
      "At index 26 (restaurant 27), the restaurant name has letters: Homeslice Neal’s Yard\n",
      "At index 27 (restaurant 28), the restaurant name has letters: Yauatcha\n",
      "At index 28 (restaurant 29), the restaurant name has letters: Bocca Di Lupo\n",
      "At index 29 (restaurant 30), the restaurant name has letters: Abeno\n",
      "At index 30 (restaurant 31), the restaurant name has letters: Honey & Co\n",
      "At index 31 (restaurant 32), the restaurant name has letters: Kiln\n",
      "At index 32 (restaurant 33), the restaurant name has letters: Bibimbap\n",
      "At index 33 (restaurant 34), the restaurant name has letters: The Rum Kitchen\n",
      "At index 34 (restaurant 35), the restaurant name has letters: Busaba Soho\n",
      "At index 35 (restaurant 36), the restaurant name has letters: Savoir Faire\n",
      "At index 36 (restaurant 37), the restaurant name has letters: The Ledbury\n",
      "At index 37 (restaurant 38), the restaurant name has letters: Shoryu Ramen\n",
      "At index 38 (restaurant 39), the restaurant name has letters: Jinjuu\n",
      "At index 39 (restaurant 40), the restaurant name has letters: Regency Café\n",
      "At index 40 (restaurant 41), the restaurant name has letters: The Wolseley\n",
      "At index 41 (restaurant 42), the restaurant name has letters: Gordon Ramsay Street Pizza\n",
      "At index 42 (restaurant 43), the restaurant name has letters: Silk Road\n",
      "At index 43 (restaurant 44), the restaurant name has letters: Misato\n",
      "At index 44 (restaurant 45), the restaurant name has letters: San Carlo Cicchetti\n",
      "At index 45 (restaurant 46), the restaurant name has letters: Barrafina\n",
      "At index 46 (restaurant 47), the restaurant name has letters: Soho Joe\n",
      "At index 47 (restaurant 48), the restaurant name has letters: The Orangery\n",
      "At index 48 (restaurant 49), the restaurant name has letters: Nando’s\n",
      "At index 49 (restaurant 50), the restaurant name has letters: Bodean’s\n",
      "At index 50 (restaurant 51), the restaurant name has letters: The Alchemist\n",
      "At index 51 (restaurant 52), the restaurant name has letters: Circolo Popolare\n",
      "At index 52 (restaurant 53), the restaurant name has letters: Ye Olde Cheshire Cheese\n",
      "At index 53 (restaurant 54), the restaurant name has letters: The Grazing Goat\n",
      "At index 54 (restaurant 55), the restaurant name has letters: Kanada-Ya\n",
      "At index 55 (restaurant 56), the restaurant name has letters: Dominique Ansel Treehouse\n",
      "At index 56 (restaurant 57), the restaurant name has letters: Bageriet\n",
      "At index 57 (restaurant 58), the restaurant name has letters: Where the Pancakes are\n",
      "At index 58 (restaurant 59), the restaurant name has letters: Hoppers\n",
      "At index 59 (restaurant 60), the restaurant name has letters: The English Rose Cafe and Tea Shop\n",
      "At index 60 (restaurant 61), the restaurant name is a number: 10\n",
      "At index 61 (restaurant 62), the restaurant name has letters: Koya Soho\n",
      "At index 62 (restaurant 63), the restaurant name has letters: Dinner by Heston Blumenthal\n",
      "At index 63 (restaurant 64), the restaurant name has letters: Burger & Lobster\n",
      "At index 64 (restaurant 65), the restaurant name has letters: Barrafina\n",
      "At index 65 (restaurant 66), the restaurant name has letters: Yalla Yalla\n",
      "At index 66 (restaurant 67), the restaurant name has letters: Piccolino\n",
      "At index 67 (restaurant 68), the restaurant name has letters: Maria Sabina\n",
      "At index 68 (restaurant 69), the restaurant name has letters: Flat Iron\n",
      "At index 69 (restaurant 70), the restaurant name has letters: The Victoria\n",
      "At index 70 (restaurant 71), the restaurant name has letters: Roti King\n",
      "At index 71 (restaurant 72), the restaurant name is a number: 10\n",
      "At index 72 (restaurant 73), the restaurant name is a number: 11\n",
      "At index 73 (restaurant 74), the restaurant name has letters: Tayyabs\n",
      "At index 74 (restaurant 75), the restaurant name has letters: The Ivy\n",
      "At index 75 (restaurant 76), the restaurant name has letters: Da Mario Restaurant\n",
      "At index 76 (restaurant 77), the restaurant name has letters: Wok to Walk - Oxford St\n",
      "At index 77 (restaurant 78), the restaurant name has letters: Patara\n",
      "At index 78 (restaurant 79), the restaurant name has letters: Kazu\n",
      "At index 79 (restaurant 80), the restaurant name has letters: Mr Fogg’s Tavern\n",
      "At index 80 (restaurant 81), the restaurant name has letters: Duck & Waffle Local\n",
      "At index 81 (restaurant 82), the restaurant name has letters: Naru\n",
      "At index 82 (restaurant 83), the restaurant name has letters: Absurd Bird Soho\n",
      "At index 83 (restaurant 84), the restaurant name is a number: 10\n",
      "At index 84 (restaurant 85), the restaurant name is a number: 11\n",
      "At index 85 (restaurant 86), the restaurant name is a number: 12\n",
      "At index 86 (restaurant 87), the restaurant name has letters: Mister Lasagna\n",
      "At index 87 (restaurant 88), the restaurant name has letters: Bailey’s Fish and Chips\n",
      "At index 88 (restaurant 89), the restaurant name has letters: Seoul Bakery\n",
      "At index 89 (restaurant 90), the restaurant name has letters: Honey & Smoke\n",
      "At index 90 (restaurant 91), the restaurant name has letters: Golden Union Fish Bar\n",
      "At index 91 (restaurant 92), the restaurant name has letters: The Table\n",
      "At index 92 (restaurant 93), the restaurant name has letters: Lupita\n",
      "At index 93 (restaurant 94), the restaurant name has letters: Hawksmoor\n",
      "At index 94 (restaurant 95), the restaurant name has letters: Burger & Lobster\n",
      "At index 95 (restaurant 96), the restaurant name has letters: Feya\n",
      "At index 96 (restaurant 97), the restaurant name is a number: 10\n",
      "At index 97 (restaurant 98), the restaurant name is a number: 11\n",
      "At index 98 (restaurant 99), the restaurant name is a number: 12\n",
      "At index 99 (restaurant 100), the restaurant name is a number: 13\n",
      "At index 100 (restaurant 101), the restaurant name has letters: Fuckoffee\n",
      "At index 101 (restaurant 102), the restaurant name has letters: Nando’s\n",
      "At index 102 (restaurant 103), the restaurant name has letters: Darjeeling Express\n",
      "At index 103 (restaurant 104), the restaurant name has letters: Bill’s\n",
      "At index 104 (restaurant 105), the restaurant name has letters: ROVI\n",
      "At index 105 (restaurant 106), the restaurant name has letters: Cecconi’s Mayfair\n",
      "At index 106 (restaurant 107), the restaurant name has letters: Laksamania\n",
      "At index 107 (restaurant 108), the restaurant name has letters: The Barbary\n",
      "At index 108 (restaurant 109), the restaurant name has letters: La Porchetta Pollo Bar\n",
      "At index 109 (restaurant 110), the restaurant name has letters: Belgo Centraal\n",
      "At index 110 (restaurant 111), the restaurant name is a number: 11\n",
      "At index 111 (restaurant 112), the restaurant name is a number: 12\n",
      "At index 112 (restaurant 113), the restaurant name is a number: 13\n",
      "At index 113 (restaurant 114), the restaurant name is a number: 14\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(london_yelp_restaurants['Name'])):\n",
    "    if london_yelp_restaurants['Name'][i].isdigit():\n",
    "        print(f\"At index {i} (restaurant {i + 1}), the restaurant name is a number: {london_yelp_restaurants['Name'][i]}\")\n",
    "    else:\n",
    "        print(f\"At index {i} (restaurant {i + 1}), the restaurant name has letters: {london_yelp_restaurants['Name'][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa, whoa, whoa!  Starting on page 2, the titles are all out of order!  The order on the Yelp page itself does **not** match the order in the list above.  And some on the page are even skipped altogether!\n",
    "\n",
    "(Aside: part of this phenomenon was due to the fact that Yelp's ranking was changing in real time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess this shows the value of spot-checking.  Augh.  I guess it's time to revise the logic of the restaurant name finder, and then I need to check the other fields to make sure they're good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relatedly, this is the major disadvantage of putting everything in one block...it's tough to troubleshoot!  I guess I shouldn't fight against the small-batch nature of Jupyter.  That's the point!\n",
    "\n",
    "Splitting it back up and troubleshooting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's generate the URLs we want to look at\n",
    "base_url = \"https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1\"\n",
    "\n",
    "page_start_value = 90\n",
    "\n",
    "url_to_scrape = f\"{base_url}&start={page_start_value}\"  # append a suffix to the base url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So let's get to requestin'!\n",
    "http_response = requests.get(url_to_scrape)   # this actually requests the page and stores the resulting response\n",
    "\n",
    "# get the text version of the http_response\n",
    "source_code_text = http_response.text   # the .text method is part of the requests library, not BeautifulSoup\n",
    "\n",
    "# then we'll give that text source code to BeautifulSoup, which creates an object with useful scraping methods\n",
    "yelp_scrape = BeautifulSoup(source_code_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the Dictionary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to structure this data as a dictionary of lists\n",
    "london_yelp_restaurants = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape for Names\n",
    "I revised the logic in the cleaning part because it was allowing numbers where we didn't want them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's scrape for names:\n",
    "scraped_names = yelp_scrape.find_all('a', {'class': 'css-166la90'})  # this returns a list of tags that match the criteria\n",
    "\n",
    "# scraped_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text\n",
    "scraped_names = [scraped_name.text for scraped_name in scraped_names]\n",
    "\n",
    "# scraped_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean it up by filtering out empty strings and numbers \n",
    "# this logic will be a problem if a restaurant's name is just a number...\n",
    "for name in scraped_names:\n",
    "    scraped_names = [name for name in scraped_names if name != '' and not name.isdigit()]\n",
    "\n",
    "# scraped_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add it to the dictionary\n",
    "london_yelp_restaurants['Name'] = scraped_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape for Ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's scrape for ranks:\n",
    "scraped_ranks = yelp_scrape.find_all('span', {'class': 'css-1pxmz4g'})\n",
    "\n",
    "# get the text\n",
    "scraped_ranks = [rank.text for rank in scraped_ranks]  # get the text between the tags\n",
    "\n",
    "# now parse out the number\n",
    "scraped_ranks = [rank[0:rank.index('.')] for rank in scraped_ranks]  # grab the first characters before the \".\"\n",
    "\n",
    "# let's assume we want integers, so convert\n",
    "scraped_ranks = [int(rank) for rank in scraped_ranks]\n",
    "\n",
    "# add it to the dictionary\n",
    "london_yelp_restaurants['Rank'] = scraped_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape for Price Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's scrape for price ranges.  This one got tricky, and we had to go for an outside-in approach.\n",
    "scraped_price_ranges = yelp_scrape.find_all('div', {'class': 'priceCategory__09f24__2IbAM'})\n",
    "\n",
    "# scraped_price_ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text\n",
    "scraped_price_ranges = [price.text for price in scraped_price_ranges]\n",
    "\n",
    "# scraped_price_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now grab the £ signs if they're there, otherwise `None`\n",
    "for element in range(len(scraped_price_ranges)):\n",
    "    if scraped_price_ranges[element].count(\"\\xA3\") == 0:\n",
    "        scraped_price_ranges[element] = None\n",
    "    else:\n",
    "        scraped_price_ranges[element] = scraped_price_ranges[element].count(\"\\xA3\")\n",
    "\n",
    "# scraped_price_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add it to the dictionary\n",
    "london_yelp_restaurants['Price range'] = scraped_price_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape for Star Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's scrape for star rating:\n",
    "scraped_star_ratings = yelp_scrape.find_all('div', {'class': 'i-stars__09f24__1T6rz'})\n",
    "\n",
    "# narrow it down\n",
    "scraped_star_ratings = [rating.attrs['aria-label'] for rating in scraped_star_ratings]\n",
    "\n",
    "# clean it up by getting rid of the \"star rating\" suffix and converting to a float\n",
    "scraped_star_ratings = [float(rating[0:len(rating) - len(\" star rating\")]) for rating in scraped_star_ratings]\n",
    "\n",
    "# add it to the dictionary\n",
    "london_yelp_restaurants['Star rating'] = scraped_star_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape for Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's scrape for neighborhood:\n",
    "scraped_neighborhood_containers = yelp_scrape.find_all('div', {'class': 'container__09f24__1fWZl'})\n",
    "\n",
    "# now let's just get the <p> tags within each container\n",
    "scraped_neighborhoods = [container.find_all('p') for container in scraped_neighborhood_containers]\n",
    "\n",
    "# let's simplify what we're looking at by getting text of each tag\n",
    "for collection in scraped_neighborhoods:\n",
    "    for tag in range(0,len(collection)):\n",
    "        collection[tag] = collection[tag].text   # is there a more efficienty way to do this?\n",
    "\n",
    "# now let's get the last element of each list\n",
    "scraped_neighborhoods = [location[len(location)-1] for location in scraped_neighborhoods]\n",
    "\n",
    "# add it to the dictionary\n",
    "london_yelp_restaurants['Neighborhood'] = scraped_neighborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Results for Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: Name\n",
      "10 elements:\n",
      "['Fuckoffee', 'Nando’s', 'Darjeeling Express', 'Bill’s', 'ROVI', 'Cecconi’s Mayfair', 'Laksamania', 'The Barbary', 'La Porchetta Pollo Bar', 'Belgo Centraal']\n",
      "\n",
      "Key: Rank\n",
      "10 elements:\n",
      "[91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n",
      "\n",
      "Key: Price range\n",
      "10 elements:\n",
      "[1, 2, None, 2, None, 3, None, 3, 1, 2]\n",
      "\n",
      "Key: Star rating\n",
      "10 elements:\n",
      "[4.0, 4.0, 4.5, 4.0, 4.0, 4.0, 4.5, 4.5, 4.0, 4.0]\n",
      "\n",
      "Key: Neighborhood\n",
      "10 elements:\n",
      "['Borough', 'South Kensington', 'Soho', 'Soho', 'Fitzrovia', 'Mayfair', 'Fitzrovia', 'Covent Garden', 'Bloomsbury', 'Covent Garden']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's see what we got!\n",
    "for key in london_yelp_restaurants:\n",
    "    print(f\"Key: {key}\")\n",
    "    if len(london_yelp_restaurants[key]) == 10:\n",
    "        print(f\"{len(london_yelp_restaurants[key])} elements:\")\n",
    "    else:\n",
    "        print(f\"PROBLEM >>> {len(london_yelp_restaurants[key])} elements:\")\n",
    "    print(london_yelp_restaurants[key])\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "# and let's load it into a dataframe\n",
    "# pd.DataFrame(london_yelp_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Results into a DataFrame\n",
    "Note: this will throw an error if the lists above are not the same length!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Price range</th>\n",
       "      <th>Star rating</th>\n",
       "      <th>Neighborhood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fuckoffee</td>\n",
       "      <td>91</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Borough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nando’s</td>\n",
       "      <td>92</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>South Kensington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Darjeeling Express</td>\n",
       "      <td>93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bill’s</td>\n",
       "      <td>94</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ROVI</td>\n",
       "      <td>95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Fitzrovia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cecconi’s Mayfair</td>\n",
       "      <td>96</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Mayfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Laksamania</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Fitzrovia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Barbary</td>\n",
       "      <td>98</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Covent Garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>La Porchetta Pollo Bar</td>\n",
       "      <td>99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bloomsbury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Belgo Centraal</td>\n",
       "      <td>100</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Covent Garden</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Name  Rank  Price range  Star rating      Neighborhood\n",
       "0               Fuckoffee    91          1.0          4.0           Borough\n",
       "1                 Nando’s    92          2.0          4.0  South Kensington\n",
       "2      Darjeeling Express    93          NaN          4.5              Soho\n",
       "3                  Bill’s    94          2.0          4.0              Soho\n",
       "4                    ROVI    95          NaN          4.0         Fitzrovia\n",
       "5       Cecconi’s Mayfair    96          3.0          4.0           Mayfair\n",
       "6              Laksamania    97          NaN          4.5         Fitzrovia\n",
       "7             The Barbary    98          3.0          4.5     Covent Garden\n",
       "8  La Porchetta Pollo Bar    99          1.0          4.0        Bloomsbury\n",
       "9          Belgo Centraal   100          2.0          4.0     Covent Garden"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and let's load it into a dataframe\n",
    "pd.DataFrame(london_yelp_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so after all that troubleshooting, it seems to be working!  Let's put it back into a cell-by-cell solution that iterates through the ten pages!\n",
    "\n",
    "I also learned at this point that it's easy to merge and split cells in a Jupyter notebook, so I'm going to re-merge everything related to the scraping and put it into a function definition so it's easy to iterate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the scraping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_a_page(page_to_scrape):\n",
    "    # So let's get to requestin'!\n",
    "    http_response = requests.get(page_to_scrape)   # this actually requests the page and stores the resulting response\n",
    "\n",
    "    # get the text version of the http_response\n",
    "    source_code_text = http_response.text   # the .text method is part of the requests library, not BeautifulSoup\n",
    "\n",
    "    # then we'll give that text source code to BeautifulSoup, which creates an object with useful scraping methods\n",
    "    yelp_scrape = BeautifulSoup(source_code_text)\n",
    "\n",
    "    #### Initialize the Dictionary of Results\n",
    "    # we're going to structure this data as a dictionary of lists\n",
    "    london_yelp_restaurants = {}\n",
    "\n",
    "    #### Scrape for Names\n",
    "    # I revised the logic in the cleaning part because it was allowing numbers where we didn't want them.\n",
    "    # let's scrape for names:\n",
    "    scraped_names = yelp_scrape.find_all('a', {'class': 'css-166la90'})  # this returns a list of tags that match the criteria\n",
    "\n",
    "    # scraped_names\n",
    "\n",
    "    # get the text\n",
    "    scraped_names = [scraped_name.text for scraped_name in scraped_names]\n",
    "\n",
    "    # scraped_names\n",
    "\n",
    "    # clean it up by filtering out empty strings and numbers \n",
    "    # this logic will be a problem if a restaurant's name is just a number...\n",
    "    for name in scraped_names:\n",
    "        scraped_names = [name for name in scraped_names if name != '' and not name.isdigit()]\n",
    "\n",
    "    # scraped_names\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Name'] = scraped_names\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Ranks\n",
    "    # now let's scrape for ranks:\n",
    "    scraped_ranks = yelp_scrape.find_all('span', {'class': 'css-1pxmz4g'})\n",
    "\n",
    "    # get the text\n",
    "    scraped_ranks = [rank.text for rank in scraped_ranks]  # get the text between the tags\n",
    "\n",
    "    # now parse out the number\n",
    "    scraped_ranks = [rank[0:rank.index('.')] for rank in scraped_ranks]  # grab the first characters before the \".\"\n",
    "\n",
    "    # let's assume we want integers, so convert\n",
    "    scraped_ranks = [int(rank) for rank in scraped_ranks]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Rank'] = scraped_ranks\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Price Ranges\n",
    "    # now let's scrape for price ranges.  This one got tricky, and we had to go for an outside-in approach.\n",
    "    scraped_price_ranges = yelp_scrape.find_all('div', {'class': 'priceCategory__09f24__2IbAM'})\n",
    "\n",
    "    # scraped_price_ranges\n",
    "\n",
    "    # get the text\n",
    "    scraped_price_ranges = [price.text for price in scraped_price_ranges]\n",
    "\n",
    "    # scraped_price_ranges\n",
    "\n",
    "    # now grab the £ signs if they're there, otherwise `None`\n",
    "    for element in range(len(scraped_price_ranges)):\n",
    "        if scraped_price_ranges[element].count(\"\\xA3\") == 0:\n",
    "            scraped_price_ranges[element] = None\n",
    "        else:\n",
    "            scraped_price_ranges[element] = scraped_price_ranges[element].count(\"\\xA3\")\n",
    "\n",
    "    # scraped_price_ranges\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Price range'] = scraped_price_ranges\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Star Ratings\n",
    "    # now let's scrape for star rating:\n",
    "    scraped_star_ratings = yelp_scrape.find_all('div', {'class': 'i-stars__09f24__1T6rz'})\n",
    "\n",
    "    # narrow it down\n",
    "    scraped_star_ratings = [rating.attrs['aria-label'] for rating in scraped_star_ratings]\n",
    "\n",
    "    # clean it up by getting rid of the \"star rating\" suffix and converting to a float\n",
    "    scraped_star_ratings = [float(rating[0:len(rating) - len(\" star rating\")]) for rating in scraped_star_ratings]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Star rating'] = scraped_star_ratings\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Neighborhood\n",
    "    # now let's scrape for neighborhood:\n",
    "    scraped_neighborhood_containers = yelp_scrape.find_all('div', {'class': 'container__09f24__1fWZl'})\n",
    "\n",
    "    # now let's just get the <p> tags within each container\n",
    "    scraped_neighborhoods = [container.find_all('p') for container in scraped_neighborhood_containers]\n",
    "\n",
    "    # let's simplify what we're looking at by getting text of each tag\n",
    "    for collection in scraped_neighborhoods:\n",
    "        for tag in range(0,len(collection)):\n",
    "            collection[tag] = collection[tag].text   # is there a more efficienty way to do this?\n",
    "\n",
    "    # now let's get the last element of each list\n",
    "    scraped_neighborhoods = [location[len(location)-1] for location in scraped_neighborhoods]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Neighborhood'] = scraped_neighborhoods\n",
    "    \n",
    "    \n",
    "    #### Return the dictionary\n",
    "    return london_yelp_restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dictionary to hold the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create a dictionary to hold the results (a dictionary of lists)\n",
    "london_yelp_restaurants = {\n",
    "    \"Name\": [],\n",
    "    \"Rank\": [],\n",
    "    \"Price range\": [],\n",
    "    \"Star rating\": [],\n",
    "    \"Neighborhood\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=0',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=10',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=20',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=30',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=40',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=50',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=60',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=70',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=80',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=90']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = \"https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1\"\n",
    "\n",
    "# generate a list like [0, 10, 20, ..., 90] to get the first ten pages (first 100 results)\n",
    "page_start_values = [i * 10 for i in range(0, 10)] \n",
    "\n",
    "# create a list of URLs to scrape\n",
    "urls_to_scrape = [f\"{base_url}&start={value}\" for value in page_start_values]   # append a suffix to the base url\n",
    "\n",
    "urls_to_scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through the pages and call the scraping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's loop through all the pages we want to scrape, and call the function\n",
    "\n",
    "for page in urls_to_scrape:  # for each site in the list of URLs to scrape\n",
    "    # call the scrape function for the page and store it in a dictionary\n",
    "    single_page_yelp_results = scrape_a_page(page)\n",
    "    \n",
    "    # now append the single-page results to the main results dictionary\n",
    "    for key in london_yelp_restaurants:\n",
    "        london_yelp_restaurants[key] += single_page_yelp_results[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Results into a DataFrame\n",
    "Note: this will throw an error if the lists above are not the same length!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Price range</th>\n",
       "      <th>Star rating</th>\n",
       "      <th>Neighborhood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Mayfair Chippy</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Mayfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Covent Garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flat Iron</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ffiona’s Restaurant</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Kensington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Cecconi’s Mayfair</td>\n",
       "      <td>96</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Mayfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Laksamania</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Fitzrovia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>The Barbary</td>\n",
       "      <td>98</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Covent Garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>La Porchetta Pollo Bar</td>\n",
       "      <td>99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bloomsbury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Belgo Centraal</td>\n",
       "      <td>100</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Covent Garden</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name  Rank  Price range  Star rating   Neighborhood\n",
       "0       The Mayfair Chippy     1          2.0          4.5        Mayfair\n",
       "1                  Dishoom     2          2.0          4.5  Covent Garden\n",
       "2                Flat Iron     3          2.0          4.5           Soho\n",
       "3      Ffiona’s Restaurant     4          2.0          4.5     Kensington\n",
       "4                  Dishoom     5          2.0          4.5           Soho\n",
       "..                     ...   ...          ...          ...            ...\n",
       "95       Cecconi’s Mayfair    96          3.0          4.0        Mayfair\n",
       "96              Laksamania    97          NaN          4.5      Fitzrovia\n",
       "97             The Barbary    98          3.0          4.5  Covent Garden\n",
       "98  La Porchetta Pollo Bar    99          1.0          4.0     Bloomsbury\n",
       "99          Belgo Centraal   100          2.0          4.0  Covent Garden\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and let's load it into a dataframe\n",
    "pd.DataFrame(london_yelp_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiers 2 and 3 - Cleaned-Up Code Answer\n",
    "By providing five columns, the below answers both tiers 2 and 3.\n",
    "\n",
    "Below is my cleaned-up code that executes everything in a single cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Price range</th>\n",
       "      <th>Star rating</th>\n",
       "      <th>Neighborhood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Mayfair Chippy</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Mayfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Covent Garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flat Iron</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ffiona’s Restaurant</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Kensington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Cecconi’s Mayfair</td>\n",
       "      <td>96</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Mayfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Laksamania</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Fitzrovia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>The Barbary</td>\n",
       "      <td>98</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Covent Garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>La Porchetta Pollo Bar</td>\n",
       "      <td>99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bloomsbury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Belgo Centraal</td>\n",
       "      <td>100</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Covent Garden</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name  Rank  Price range  Star rating   Neighborhood\n",
       "0       The Mayfair Chippy     1          2.0          4.5        Mayfair\n",
       "1                  Dishoom     2          2.0          4.5  Covent Garden\n",
       "2                Flat Iron     3          2.0          4.5           Soho\n",
       "3      Ffiona’s Restaurant     4          2.0          4.5     Kensington\n",
       "4                  Dishoom     5          2.0          4.5           Soho\n",
       "..                     ...   ...          ...          ...            ...\n",
       "95       Cecconi’s Mayfair    96          3.0          4.0        Mayfair\n",
       "96              Laksamania    97          NaN          4.5      Fitzrovia\n",
       "97             The Barbary    98          3.0          4.5  Covent Garden\n",
       "98  La Porchetta Pollo Bar    99          1.0          4.0     Bloomsbury\n",
       "99          Belgo Centraal   100          2.0          4.0  Covent Garden\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're going to need three Python libraries to make this code work:\n",
    "\n",
    "import requests   # this library will help us make http requests, which is how we get webpages\n",
    "from bs4 import BeautifulSoup   # this library will help us parse html source code (i.e., webscraping)\n",
    "import pandas as pd   # this library will help us with data science stuff\n",
    "\n",
    "\n",
    "\n",
    "def scrape_a_page(page_to_scrape):\n",
    "    # So let's get to requestin'!\n",
    "    http_response = requests.get(page_to_scrape)   # this actually requests the page and stores the resulting response\n",
    "\n",
    "    # get the text version of the http_response\n",
    "    source_code_text = http_response.text   # the .text method is part of the requests library, not BeautifulSoup\n",
    "\n",
    "    # then we'll give that text source code to BeautifulSoup, which creates an object with useful scraping methods\n",
    "    yelp_scrape = BeautifulSoup(source_code_text)\n",
    "\n",
    "    #### Initialize the Dictionary of Results\n",
    "    # we're going to structure this data as a dictionary of lists\n",
    "    london_yelp_restaurants = {}\n",
    "\n",
    "    #### Scrape for Names\n",
    "    # I revised the logic in the cleaning part because it was allowing numbers where we didn't want them.\n",
    "    # let's scrape for names:\n",
    "    scraped_names = yelp_scrape.find_all('a', {'class': 'css-166la90'})  # this returns a list of tags that match the criteria\n",
    "\n",
    "    # scraped_names\n",
    "\n",
    "    # get the text\n",
    "    scraped_names = [scraped_name.text for scraped_name in scraped_names]\n",
    "\n",
    "    # scraped_names\n",
    "\n",
    "    # clean it up by filtering out empty strings and numbers \n",
    "    # this logic will be a problem if a restaurant's name is just a number...\n",
    "    for name in scraped_names:\n",
    "        scraped_names = [name for name in scraped_names if name != '' and not name.isdigit()]\n",
    "\n",
    "    # scraped_names\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Name'] = scraped_names\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Ranks\n",
    "    # now let's scrape for ranks:\n",
    "    scraped_ranks = yelp_scrape.find_all('span', {'class': 'css-1pxmz4g'})\n",
    "\n",
    "    # get the text\n",
    "    scraped_ranks = [rank.text for rank in scraped_ranks]  # get the text between the tags\n",
    "\n",
    "    # now parse out the number\n",
    "    scraped_ranks = [rank[0:rank.index('.')] for rank in scraped_ranks]  # grab the first characters before the \".\"\n",
    "\n",
    "    # let's assume we want integers, so convert\n",
    "    scraped_ranks = [int(rank) for rank in scraped_ranks]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Rank'] = scraped_ranks\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Price Ranges\n",
    "    # now let's scrape for price ranges.  This one got tricky, and we had to go for an outside-in approach.\n",
    "    scraped_price_ranges = yelp_scrape.find_all('div', {'class': 'priceCategory__09f24__2IbAM'})\n",
    "\n",
    "    # scraped_price_ranges\n",
    "\n",
    "    # get the text\n",
    "    scraped_price_ranges = [price.text for price in scraped_price_ranges]\n",
    "\n",
    "    # scraped_price_ranges\n",
    "\n",
    "    # now grab the £ signs if they're there, otherwise `None`\n",
    "    for element in range(len(scraped_price_ranges)):\n",
    "        if scraped_price_ranges[element].count(\"\\xA3\") == 0:\n",
    "            scraped_price_ranges[element] = None\n",
    "        else:\n",
    "            scraped_price_ranges[element] = scraped_price_ranges[element].count(\"\\xA3\")\n",
    "\n",
    "    # scraped_price_ranges\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Price range'] = scraped_price_ranges\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Star Ratings\n",
    "    # now let's scrape for star rating:\n",
    "    scraped_star_ratings = yelp_scrape.find_all('div', {'class': 'i-stars__09f24__1T6rz'})\n",
    "\n",
    "    # narrow it down\n",
    "    scraped_star_ratings = [rating.attrs['aria-label'] for rating in scraped_star_ratings]\n",
    "\n",
    "    # clean it up by getting rid of the \"star rating\" suffix and converting to a float\n",
    "    scraped_star_ratings = [float(rating[0:len(rating) - len(\" star rating\")]) for rating in scraped_star_ratings]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Star rating'] = scraped_star_ratings\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Neighborhood\n",
    "    # now let's scrape for neighborhood:\n",
    "    scraped_neighborhood_containers = yelp_scrape.find_all('div', {'class': 'container__09f24__1fWZl'})\n",
    "\n",
    "    # now let's just get the <p> tags within each container\n",
    "    scraped_neighborhoods = [container.find_all('p') for container in scraped_neighborhood_containers]\n",
    "\n",
    "    # let's simplify what we're looking at by getting text of each tag\n",
    "    for collection in scraped_neighborhoods:\n",
    "        for tag in range(0,len(collection)):\n",
    "            collection[tag] = collection[tag].text   # is there a more efficienty way to do this?\n",
    "\n",
    "    # now let's get the last element of each list\n",
    "    scraped_neighborhoods = [location[len(location)-1] for location in scraped_neighborhoods]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Neighborhood'] = scraped_neighborhoods\n",
    "    \n",
    "    \n",
    "    #### Return the dictionary\n",
    "    return london_yelp_restaurants\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create a dictionary to hold the results (a dictionary of lists)\n",
    "london_yelp_restaurants = {\n",
    "    \"Name\": [],\n",
    "    \"Rank\": [],\n",
    "    \"Price range\": [],\n",
    "    \"Star rating\": [],\n",
    "    \"Neighborhood\": []\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate the URLs\n",
    "base_url = \"https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1\"\n",
    "\n",
    "# generate a list like [0, 10, 20, ..., 90] to get the first ten pages (first 100 results)\n",
    "page_start_values = [i * 10 for i in range(0,10)] \n",
    "\n",
    "# create a list of URLs to scrape\n",
    "urls_to_scrape = [f\"{base_url}&start={value}\" for value in page_start_values]   # append a suffix to the base url\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loop through the pages and call the scraping function\n",
    "\n",
    "for page in urls_to_scrape:  # for each site in the list of URLs to scrape\n",
    "    # call the scrape function for the page and store it in a dictionary\n",
    "    single_page_yelp_results = scrape_a_page(page)\n",
    "    \n",
    "    # now append the single-page results to the main results dictionary\n",
    "    for key in london_yelp_restaurants:\n",
    "        london_yelp_restaurants[key] += single_page_yelp_results[key]\n",
    "\n",
    "        \n",
    "        \n",
    "# Load the results into a DataFrame\n",
    "# Note: this will throw an error if the lists above are not the same length!\n",
    "\n",
    "pd.DataFrame(london_yelp_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier 4: 100 Row Dataset With At Least 5 Columns + Individual Restaurant Categories\n",
    "- First is me showing my work\n",
    "- Below that is my cleaned-up, all-in-one-cell code block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 4 - Showing My Work\n",
    "- Building off the Tier 2 and 3 code, now we want to bring in the category data.  There may be any number of categories, so including zero, so we'll want to import a list of them for each restaurant.\n",
    "- When we have the returned results, we'll want to split the restaurant categories into their own columns, and put a `None` if a given restaurant has fewer than the maximum number of categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's scrape the list of categories for each restaurant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's generate the URLs we want to look at\n",
    "base_url = \"https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1\"\n",
    "\n",
    "page_start_value = 70\n",
    "\n",
    "url_to_scrape = f\"{base_url}&start={page_start_value}\"  # append a suffix to the base url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So let's get to requestin'!\n",
    "http_response = requests.get(url_to_scrape)   # this actually requests the page and stores the resulting response\n",
    "\n",
    "# get the text version of the http_response\n",
    "source_code_text = http_response.text   # the .text method is part of the requests library, not BeautifulSoup\n",
    "\n",
    "# then we'll give that text source code to BeautifulSoup, which creates an object with useful scraping methods\n",
    "yelp_scrape = BeautifulSoup(source_code_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the Dictionary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to structure this data as a dictionary of lists\n",
    "london_yelp_restaurants = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape for Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's scrape for names:\n",
    "scraped_names = yelp_scrape.find_all('a', {'class': 'css-166la90'})  # this returns a list of tags that match the criteria\n",
    "\n",
    "# scraped_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text\n",
    "scraped_names = [scraped_name.text for scraped_name in scraped_names]\n",
    "\n",
    "# scraped_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean it up by filtering out empty strings and numbers \n",
    "# this logic will be a problem if a restaurant's name is just a number...\n",
    "for name in scraped_names:\n",
    "    scraped_names = [name for name in scraped_names if name != '' and not name.isdigit()]\n",
    "\n",
    "# scraped_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add it to the dictionary\n",
    "london_yelp_restaurants['Name'] = scraped_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape for Ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's scrape for ranks:\n",
    "scraped_ranks = yelp_scrape.find_all('span', {'class': 'css-1pxmz4g'})\n",
    "\n",
    "# get the text\n",
    "scraped_ranks = [rank.text for rank in scraped_ranks]  # get the text between the tags\n",
    "\n",
    "# now parse out the number\n",
    "scraped_ranks = [rank[0:rank.index('.')] for rank in scraped_ranks]  # grab the first characters before the \".\"\n",
    "\n",
    "# let's assume we want integers, so convert\n",
    "scraped_ranks = [int(rank) for rank in scraped_ranks]\n",
    "\n",
    "# add it to the dictionary\n",
    "london_yelp_restaurants['Rank'] = scraped_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape for Price Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's scrape for price ranges.  This one got tricky, and we had to go for an outside-in approach.\n",
    "scraped_price_ranges = yelp_scrape.find_all('div', {'class': 'priceCategory__09f24__2IbAM'})\n",
    "\n",
    "# scraped_price_ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text\n",
    "scraped_price_ranges = [price.text for price in scraped_price_ranges]\n",
    "\n",
    "# scraped_price_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now grab the £ signs if they're there, otherwise `None`\n",
    "for element in range(len(scraped_price_ranges)):\n",
    "    if scraped_price_ranges[element].count(\"\\xA3\") == 0:\n",
    "        scraped_price_ranges[element] = None\n",
    "    else:\n",
    "        scraped_price_ranges[element] = scraped_price_ranges[element].count(\"\\xA3\")\n",
    "\n",
    "# scraped_price_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add it to the dictionary\n",
    "london_yelp_restaurants['Price range'] = scraped_price_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape for Star Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's scrape for star rating:\n",
    "scraped_star_ratings = yelp_scrape.find_all('div', {'class': 'i-stars__09f24__1T6rz'})\n",
    "\n",
    "# narrow it down\n",
    "scraped_star_ratings = [rating.attrs['aria-label'] for rating in scraped_star_ratings]\n",
    "\n",
    "# clean it up by getting rid of the \"star rating\" suffix and converting to a float\n",
    "scraped_star_ratings = [float(rating[0:len(rating) - len(\" star rating\")]) for rating in scraped_star_ratings]\n",
    "\n",
    "# add it to the dictionary\n",
    "london_yelp_restaurants['Star rating'] = scraped_star_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape for Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's scrape for neighborhood:\n",
    "scraped_neighborhood_containers = yelp_scrape.find_all('div', {'class': 'container__09f24__1fWZl'})\n",
    "\n",
    "# now let's just get the <p> tags within each container\n",
    "scraped_neighborhoods = [container.find_all('p') for container in scraped_neighborhood_containers]\n",
    "\n",
    "# let's simplify what we're looking at by getting text of each tag\n",
    "for collection in scraped_neighborhoods:\n",
    "    for tag in range(0,len(collection)):\n",
    "        collection[tag] = collection[tag].text   # is there a more efficienty way to do this?\n",
    "\n",
    "# now let's get the last element of each list\n",
    "scraped_neighborhoods = [location[len(location)-1] for location in scraped_neighborhoods]\n",
    "\n",
    "# add it to the dictionary\n",
    "london_yelp_restaurants['Neighborhood'] = scraped_neighborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape for Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's scrape for categories\n",
    "scraped_categories = yelp_scrape.find_all('p', {'class': 'css-n6i4z7'})\n",
    "\n",
    "# scraped_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['££Indian, Pakistani, Halal',\n",
       " '£££British, Modern European',\n",
       " '££Italian',\n",
       " '£Thai, Asian Fusion, Vegetarian',\n",
       " '£££Thai',\n",
       " 'Japanese, Sushi Bars',\n",
       " '££British, Pubs',\n",
       " '££Bars, Waffles',\n",
       " '££Korean',\n",
       " '£££Chicken Shop, American (New), Soul Food',\n",
       " 'Anyone experience bad service at Indian restaurants?',\n",
       " 'Can someone please suggest which restaurants in Chinatown you would recommend? And also feel free to say which restaurants you would avoid in that…',\n",
       " 'Can someone please suggest which restaurants in Chinatown you would recommend? And also feel free to say which restaurants you would avoid in that…']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the text\n",
    "scraped_categories = [categories.text for categories in scraped_categories]\n",
    "\n",
    "scraped_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Indian, Pakistani, Halal',\n",
       " 'British, Modern European',\n",
       " 'Italian',\n",
       " 'Thai, Asian Fusion, Vegetarian',\n",
       " 'Thai',\n",
       " 'Japanese, Sushi Bars',\n",
       " 'British, Pubs',\n",
       " 'Bars, Waffles',\n",
       " 'Korean',\n",
       " 'Chicken Shop, American (New), Soul Food']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now clean it up!  \n",
    "\n",
    "# Let's assume for now that the ten restaurant-specific results will be at the top.  This could be problematic later.\n",
    "scraped_categories = scraped_categories[0:10]    # get the first ten results only\n",
    "\n",
    "# now clean off any £ symbols (some strings may have none)\n",
    "for categories in range(len(scraped_categories)):\n",
    "    scraped_categories[categories] = scraped_categories[categories].replace('\\xA3', '')\n",
    "\n",
    "scraped_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Indian', ' Pakistani', ' Halal'],\n",
       " ['British', ' Modern European'],\n",
       " ['Italian'],\n",
       " ['Thai', ' Asian Fusion', ' Vegetarian'],\n",
       " ['Thai'],\n",
       " ['Japanese', ' Sushi Bars'],\n",
       " ['British', ' Pubs'],\n",
       " ['Bars', ' Waffles'],\n",
       " ['Korean'],\n",
       " ['Chicken Shop', ' American (New)', ' Soul Food']]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now split the remaining strings into lists\n",
    "for element in range(len(scraped_categories)):\n",
    "    scraped_categories[element] = scraped_categories[element].split(',')\n",
    "    \n",
    "scraped_categories    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Indian', 'Pakistani', 'Halal'],\n",
       " ['British', 'Modern European'],\n",
       " ['Italian'],\n",
       " ['Thai', 'Asian Fusion', 'Vegetarian'],\n",
       " ['Thai'],\n",
       " ['Japanese', 'Sushi Bars'],\n",
       " ['British', 'Pubs'],\n",
       " ['Bars', 'Waffles'],\n",
       " ['Korean'],\n",
       " ['Chicken Shop', 'American (New)', 'Soul Food']]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are still leading spaces on strings after the split, so trim those off\n",
    "for element in scraped_categories:\n",
    "    for category in range(len(element)):\n",
    "        if element[category][0] == \" \":\n",
    "            element[category] = element[category][1:]\n",
    "\n",
    "scraped_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Indian', 'Pakistani', 'Halal'],\n",
       " ['British', 'Modern European'],\n",
       " ['Italian'],\n",
       " ['Thai', 'Asian Fusion', 'Vegetarian'],\n",
       " ['Thai'],\n",
       " ['Japanese', 'Sushi Bars'],\n",
       " ['British', 'Pubs'],\n",
       " ['Bars', 'Waffles'],\n",
       " ['Korean'],\n",
       " ['Chicken Shop', 'American (New)', 'Soul Food']]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add it to the dictionary\n",
    "london_yelp_restaurants['Categories'] = scraped_categories\n",
    "\n",
    "london_yelp_restaurants['Categories']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the category lists into their own individual columns\n",
    "After we iterate through all the pages, we can find the maximum number of categories we encountered.  We'll make each restaurant's list be the same length by \"padding\" with `None` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the max number of categories\n",
    "max_categories = 0\n",
    "for category_list in london_yelp_restaurants['Categories']:\n",
    "    if len(category_list) > max_categories:\n",
    "        max_categories = len(category_list)\n",
    "        \n",
    "max_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['British', 'Modern European', None]\n",
      "['Italian', None]\n",
      "['Italian', None, None]\n",
      "['Thai', None]\n",
      "['Thai', None, None]\n",
      "['Japanese', 'Sushi Bars', None]\n",
      "['British', 'Pubs', None]\n",
      "['Bars', 'Waffles', None]\n",
      "['Korean', None]\n",
      "['Korean', None, None]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Indian', 'Pakistani', 'Halal'],\n",
       " ['British', 'Modern European', None],\n",
       " ['Italian', None, None],\n",
       " ['Thai', 'Asian Fusion', 'Vegetarian'],\n",
       " ['Thai', None, None],\n",
       " ['Japanese', 'Sushi Bars', None],\n",
       " ['British', 'Pubs', None],\n",
       " ['Bars', 'Waffles', None],\n",
       " ['Korean', None, None],\n",
       " ['Chicken Shop', 'American (New)', 'Soul Food']]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we pad the shorter lists with `None` values so they're all the same length\n",
    "for category_list in london_yelp_restaurants['Categories']:\n",
    "    if len(category_list) < max_categories:   # if the restaurant's category list is shorter\n",
    "        while len(category_list) < max_categories:\n",
    "            category_list.append(None)    # keep appending None to the end of the category list until it's long enough\n",
    "            print(category_list)\n",
    "\n",
    "london_yelp_restaurants['Categories']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to break up these equal-length lists into their own categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category_1\n",
      "Category_2\n",
      "Category_3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Name': ['Tayyabs',\n",
       "  'The Ivy',\n",
       "  'Da Mario Restaurant',\n",
       "  'Wok to Walk - Oxford St',\n",
       "  'Patara',\n",
       "  'Kazu',\n",
       "  'Mr Fogg’s Tavern',\n",
       "  'Duck & Waffle Local',\n",
       "  'Naru',\n",
       "  'Absurd Bird Soho'],\n",
       " 'Rank': [71, 72, 73, 74, 75, 76, 77, 78, 79, 80],\n",
       " 'Price range': [2, 3, 2, 1, 3, None, 2, 2, 2, 3],\n",
       " 'Star rating': [4.0, 4.0, 4.5, 4.5, 4.0, 5.0, 4.0, 4.5, 4.0, 4.0],\n",
       " 'Neighborhood': ['Whitechapel',\n",
       "  'Covent Garden',\n",
       "  'Kensington',\n",
       "  'Soho',\n",
       "  'Bloomsbury',\n",
       "  'Fitzrovia',\n",
       "  'Covent Garden',\n",
       "  'Leicester Square',\n",
       "  'Covent Garden',\n",
       "  'Soho'],\n",
       " 'Categories': [['Indian', 'Pakistani', 'Halal'],\n",
       "  ['British', 'Modern European', None],\n",
       "  ['Italian', None, None],\n",
       "  ['Thai', 'Asian Fusion', 'Vegetarian'],\n",
       "  ['Thai', None, None],\n",
       "  ['Japanese', 'Sushi Bars', None],\n",
       "  ['British', 'Pubs', None],\n",
       "  ['Bars', 'Waffles', None],\n",
       "  ['Korean', None, None],\n",
       "  ['Chicken Shop', 'American (New)', 'Soul Food']],\n",
       " 'Category_1': [],\n",
       " 'Category_2': [],\n",
       " 'Category_3': []}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll create one new dictionary key for each category number\n",
    "for i in range(max_categories):\n",
    "    column_name = f\"Category_{i + 1}\"\n",
    "    print(column_name)\n",
    "    london_yelp_restaurants[column_name] = []\n",
    "    \n",
    "london_yelp_restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': ['Tayyabs',\n",
       "  'The Ivy',\n",
       "  'Da Mario Restaurant',\n",
       "  'Wok to Walk - Oxford St',\n",
       "  'Patara',\n",
       "  'Kazu',\n",
       "  'Mr Fogg’s Tavern',\n",
       "  'Duck & Waffle Local',\n",
       "  'Naru',\n",
       "  'Absurd Bird Soho'],\n",
       " 'Rank': [71, 72, 73, 74, 75, 76, 77, 78, 79, 80],\n",
       " 'Price range': [2, 3, 2, 1, 3, None, 2, 2, 2, 3],\n",
       " 'Star rating': [4.0, 4.0, 4.5, 4.5, 4.0, 5.0, 4.0, 4.5, 4.0, 4.0],\n",
       " 'Neighborhood': ['Whitechapel',\n",
       "  'Covent Garden',\n",
       "  'Kensington',\n",
       "  'Soho',\n",
       "  'Bloomsbury',\n",
       "  'Fitzrovia',\n",
       "  'Covent Garden',\n",
       "  'Leicester Square',\n",
       "  'Covent Garden',\n",
       "  'Soho'],\n",
       " 'Categories': [['Indian', 'Pakistani', 'Halal'],\n",
       "  ['British', 'Modern European', None],\n",
       "  ['Italian', None, None],\n",
       "  ['Thai', 'Asian Fusion', 'Vegetarian'],\n",
       "  ['Thai', None, None],\n",
       "  ['Japanese', 'Sushi Bars', None],\n",
       "  ['British', 'Pubs', None],\n",
       "  ['Bars', 'Waffles', None],\n",
       "  ['Korean', None, None],\n",
       "  ['Chicken Shop', 'American (New)', 'Soul Food']],\n",
       " 'Category_1': ['Indian',\n",
       "  'British',\n",
       "  'Italian',\n",
       "  'Thai',\n",
       "  'Thai',\n",
       "  'Japanese',\n",
       "  'British',\n",
       "  'Bars',\n",
       "  'Korean',\n",
       "  'Chicken Shop'],\n",
       " 'Category_2': ['Pakistani',\n",
       "  'Modern European',\n",
       "  None,\n",
       "  'Asian Fusion',\n",
       "  None,\n",
       "  'Sushi Bars',\n",
       "  'Pubs',\n",
       "  'Waffles',\n",
       "  None,\n",
       "  'American (New)'],\n",
       " 'Category_3': ['Halal',\n",
       "  None,\n",
       "  None,\n",
       "  'Vegetarian',\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  'Soul Food']}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we'll populate the new category values\n",
    "# we want this to be flexible so any number of categories will work.  \n",
    "# So avoid hard-coding variables like 'Category_2'\n",
    "\n",
    "# iterate through each category list in the dictionary and place the elements into the appropriate individual category key\n",
    "for category_list in london_yelp_restaurants['Categories']:\n",
    "    for category_number in range(max_categories):\n",
    "        london_yelp_restaurants[f\"Category_{category_number + 1}\"].append(category_list[category_number])\n",
    "\n",
    "london_yelp_restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll eventually want to delete the 'Categories' key/value pair from the dictionary, but for now, let's keep it for troubleshooting purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Results for Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: Name\n",
      "10 elements:\n",
      "['Tayyabs', 'The Ivy', 'Da Mario Restaurant', 'Wok to Walk - Oxford St', 'Patara', 'Kazu', 'Mr Fogg’s Tavern', 'Duck & Waffle Local', 'Naru', 'Absurd Bird Soho']\n",
      "\n",
      "Key: Rank\n",
      "10 elements:\n",
      "[71, 72, 73, 74, 75, 76, 77, 78, 79, 80]\n",
      "\n",
      "Key: Price range\n",
      "10 elements:\n",
      "[2, 3, 2, 1, 3, None, 2, 2, 2, 3]\n",
      "\n",
      "Key: Star rating\n",
      "10 elements:\n",
      "[4.0, 4.0, 4.5, 4.5, 4.0, 5.0, 4.0, 4.5, 4.0, 4.0]\n",
      "\n",
      "Key: Neighborhood\n",
      "10 elements:\n",
      "['Whitechapel', 'Covent Garden', 'Kensington', 'Soho', 'Bloomsbury', 'Fitzrovia', 'Covent Garden', 'Leicester Square', 'Covent Garden', 'Soho']\n",
      "\n",
      "Key: Categories\n",
      "10 elements:\n",
      "[['Indian', 'Pakistani', 'Halal'], ['British', 'Modern European', None], ['Italian', None, None], ['Thai', 'Asian Fusion', 'Vegetarian'], ['Thai', None, None], ['Japanese', 'Sushi Bars', None], ['British', 'Pubs', None], ['Bars', 'Waffles', None], ['Korean', None, None], ['Chicken Shop', 'American (New)', 'Soul Food']]\n",
      "\n",
      "Key: Category_1\n",
      "10 elements:\n",
      "['Indian', 'British', 'Italian', 'Thai', 'Thai', 'Japanese', 'British', 'Bars', 'Korean', 'Chicken Shop']\n",
      "\n",
      "Key: Category_2\n",
      "10 elements:\n",
      "['Pakistani', 'Modern European', None, 'Asian Fusion', None, 'Sushi Bars', 'Pubs', 'Waffles', None, 'American (New)']\n",
      "\n",
      "Key: Category_3\n",
      "10 elements:\n",
      "['Halal', None, None, 'Vegetarian', None, None, None, None, None, 'Soul Food']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's see what we got!\n",
    "for key in london_yelp_restaurants:\n",
    "    print(f\"Key: {key}\")\n",
    "    if len(london_yelp_restaurants[key]) == 10:\n",
    "        print(f\"{len(london_yelp_restaurants[key])} elements:\")\n",
    "    else:\n",
    "        print(f\"PROBLEM >>> {len(london_yelp_restaurants[key])} elements:\")\n",
    "    print(london_yelp_restaurants[key])\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "# and let's load it into a dataframe\n",
    "# pd.DataFrame(london_yelp_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Results into a DataFrame\n",
    "Note: this will throw an error if the lists above are not the same length!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Price range</th>\n",
       "      <th>Star rating</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Category_1</th>\n",
       "      <th>Category_2</th>\n",
       "      <th>Category_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tayyabs</td>\n",
       "      <td>71</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Whitechapel</td>\n",
       "      <td>[Indian, Pakistani, Halal]</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Pakistani</td>\n",
       "      <td>Halal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Ivy</td>\n",
       "      <td>72</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Covent Garden</td>\n",
       "      <td>[British, Modern European, None]</td>\n",
       "      <td>British</td>\n",
       "      <td>Modern European</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Da Mario Restaurant</td>\n",
       "      <td>73</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Kensington</td>\n",
       "      <td>[Italian, None, None]</td>\n",
       "      <td>Italian</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wok to Walk - Oxford St</td>\n",
       "      <td>74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Soho</td>\n",
       "      <td>[Thai, Asian Fusion, Vegetarian]</td>\n",
       "      <td>Thai</td>\n",
       "      <td>Asian Fusion</td>\n",
       "      <td>Vegetarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Patara</td>\n",
       "      <td>75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>[Thai, None, None]</td>\n",
       "      <td>Thai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kazu</td>\n",
       "      <td>76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Fitzrovia</td>\n",
       "      <td>[Japanese, Sushi Bars, None]</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>Sushi Bars</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mr Fogg’s Tavern</td>\n",
       "      <td>77</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Covent Garden</td>\n",
       "      <td>[British, Pubs, None]</td>\n",
       "      <td>British</td>\n",
       "      <td>Pubs</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Duck &amp; Waffle Local</td>\n",
       "      <td>78</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Leicester Square</td>\n",
       "      <td>[Bars, Waffles, None]</td>\n",
       "      <td>Bars</td>\n",
       "      <td>Waffles</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Naru</td>\n",
       "      <td>79</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Covent Garden</td>\n",
       "      <td>[Korean, None, None]</td>\n",
       "      <td>Korean</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Absurd Bird Soho</td>\n",
       "      <td>80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Soho</td>\n",
       "      <td>[Chicken Shop, American (New), Soul Food]</td>\n",
       "      <td>Chicken Shop</td>\n",
       "      <td>American (New)</td>\n",
       "      <td>Soul Food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name  Rank  Price range  Star rating      Neighborhood  \\\n",
       "0                  Tayyabs    71          2.0          4.0       Whitechapel   \n",
       "1                  The Ivy    72          3.0          4.0     Covent Garden   \n",
       "2      Da Mario Restaurant    73          2.0          4.5        Kensington   \n",
       "3  Wok to Walk - Oxford St    74          1.0          4.5              Soho   \n",
       "4                   Patara    75          3.0          4.0        Bloomsbury   \n",
       "5                     Kazu    76          NaN          5.0         Fitzrovia   \n",
       "6         Mr Fogg’s Tavern    77          2.0          4.0     Covent Garden   \n",
       "7      Duck & Waffle Local    78          2.0          4.5  Leicester Square   \n",
       "8                     Naru    79          2.0          4.0     Covent Garden   \n",
       "9         Absurd Bird Soho    80          3.0          4.0              Soho   \n",
       "\n",
       "                                  Categories    Category_1       Category_2  \\\n",
       "0                 [Indian, Pakistani, Halal]        Indian        Pakistani   \n",
       "1           [British, Modern European, None]       British  Modern European   \n",
       "2                      [Italian, None, None]       Italian             None   \n",
       "3           [Thai, Asian Fusion, Vegetarian]          Thai     Asian Fusion   \n",
       "4                         [Thai, None, None]          Thai             None   \n",
       "5               [Japanese, Sushi Bars, None]      Japanese       Sushi Bars   \n",
       "6                      [British, Pubs, None]       British             Pubs   \n",
       "7                      [Bars, Waffles, None]          Bars          Waffles   \n",
       "8                       [Korean, None, None]        Korean             None   \n",
       "9  [Chicken Shop, American (New), Soul Food]  Chicken Shop   American (New)   \n",
       "\n",
       "   Category_3  \n",
       "0       Halal  \n",
       "1        None  \n",
       "2        None  \n",
       "3  Vegetarian  \n",
       "4        None  \n",
       "5        None  \n",
       "6        None  \n",
       "7        None  \n",
       "8        None  \n",
       "9   Soul Food  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and let's load it into a dataframe\n",
    "pd.DataFrame(london_yelp_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!  That's just what we want.  We know it works for the first page, now let's try this approach on some other individual pages and see what speed bumps we run into.\n",
    "\n",
    "Update: after some spot checks on random single pages, it appears to be working!  So let's try iterating through all ten pages, and include removing the 'Categories' key/value pair.\n",
    "\n",
    "To make this easier, we'll put the scraping code back into its own function so it's easy to iterate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the scraping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_a_page(url_to_scrape):\n",
    "    #### Make the Request\n",
    "    # So let's get to requestin'!\n",
    "    http_response = requests.get(url_to_scrape)   # this actually requests the page and stores the resulting response\n",
    "\n",
    "    # get the text version of the http_response\n",
    "    source_code_text = http_response.text   # the .text method is part of the requests library, not BeautifulSoup\n",
    "\n",
    "    # then we'll give that text source code to BeautifulSoup, which creates an object with useful scraping methods\n",
    "    yelp_scrape = BeautifulSoup(source_code_text)\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Initialize the Dictionary of Results\n",
    "    # we're going to structure this data as a dictionary of lists\n",
    "    london_yelp_restaurants = {}\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Names\n",
    "    # let's scrape for names:\n",
    "    scraped_names = yelp_scrape.find_all('a', {'class': 'css-166la90'})  # this returns a list of tags that match the criteria\n",
    "\n",
    "    # get the text\n",
    "    scraped_names = [scraped_name.text for scraped_name in scraped_names]\n",
    "\n",
    "    # clean it up by filtering out empty strings and numbers \n",
    "    # this logic will be a problem if a restaurant's name is just a number...\n",
    "    for name in scraped_names:\n",
    "        scraped_names = [name for name in scraped_names if name != '' and not name.isdigit()]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Name'] = scraped_names\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Ranks\n",
    "    # now let's scrape for ranks:\n",
    "    scraped_ranks = yelp_scrape.find_all('span', {'class': 'css-1pxmz4g'})\n",
    "\n",
    "    # get the text\n",
    "    scraped_ranks = [rank.text for rank in scraped_ranks]  # get the text between the tags\n",
    "\n",
    "    # now parse out the number\n",
    "    scraped_ranks = [rank[0:rank.index('.')] for rank in scraped_ranks]  # grab the first characters before the \".\"\n",
    "\n",
    "    # let's assume we want integers, so convert\n",
    "    scraped_ranks = [int(rank) for rank in scraped_ranks]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Rank'] = scraped_ranks\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Price Ranges\n",
    "    # now let's scrape for price ranges.  This one got tricky, and we had to go for an outside-in approach.\n",
    "    scraped_price_ranges = yelp_scrape.find_all('div', {'class': 'priceCategory__09f24__2IbAM'})\n",
    "\n",
    "    # get the text\n",
    "    scraped_price_ranges = [price.text for price in scraped_price_ranges]\n",
    "\n",
    "    # now grab the £ signs if they're there, otherwise `None`\n",
    "    for element in range(len(scraped_price_ranges)):\n",
    "        if scraped_price_ranges[element].count(\"\\xA3\") == 0:\n",
    "            scraped_price_ranges[element] = None\n",
    "        else:\n",
    "            scraped_price_ranges[element] = scraped_price_ranges[element].count(\"\\xA3\")\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Price range'] = scraped_price_ranges\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Star Ratings\n",
    "    # now let's scrape for star rating:\n",
    "    scraped_star_ratings = yelp_scrape.find_all('div', {'class': 'i-stars__09f24__1T6rz'})\n",
    "\n",
    "    # narrow it down\n",
    "    scraped_star_ratings = [rating.attrs['aria-label'] for rating in scraped_star_ratings]\n",
    "\n",
    "    # clean it up by getting rid of the \"star rating\" suffix and converting to a float\n",
    "    scraped_star_ratings = [float(rating[0:len(rating) - len(\" star rating\")]) for rating in scraped_star_ratings]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Star rating'] = scraped_star_ratings\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Neighborhood\n",
    "    # now let's scrape for neighborhood:\n",
    "    scraped_neighborhood_containers = yelp_scrape.find_all('div', {'class': 'container__09f24__1fWZl'})\n",
    "\n",
    "    # now let's just get the <p> tags within each container\n",
    "    scraped_neighborhoods = [container.find_all('p') for container in scraped_neighborhood_containers]\n",
    "\n",
    "    # let's simplify what we're looking at by getting text of each tag\n",
    "    for collection in scraped_neighborhoods:\n",
    "        for tag in range(0,len(collection)):\n",
    "            collection[tag] = collection[tag].text   # is there a more efficienty way to do this?\n",
    "\n",
    "    # now let's get the last element of each list\n",
    "    scraped_neighborhoods = [location[len(location)-1] for location in scraped_neighborhoods]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Neighborhood'] = scraped_neighborhoods\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Categories\n",
    "    # now let's scrape for categories\n",
    "    scraped_categories = yelp_scrape.find_all('p', {'class': 'css-n6i4z7'})\n",
    "\n",
    "    # get the text\n",
    "    scraped_categories = [categories.text for categories in scraped_categories]\n",
    "\n",
    "    # now clean it up!  \n",
    "\n",
    "    # Let's assume for now that the ten restaurant-specific results will be at the top.  This could be problematic later.\n",
    "    scraped_categories = scraped_categories[0:10]    # get the first ten results only\n",
    "\n",
    "    # now clean off any £ symbols (some strings may have none)\n",
    "    for categories in range(len(scraped_categories)):\n",
    "        scraped_categories[categories] = scraped_categories[categories].replace('\\xA3', '')\n",
    "\n",
    "    # now split the remaining strings into lists\n",
    "    for element in range(len(scraped_categories)):\n",
    "        scraped_categories[element] = scraped_categories[element].split(',')\n",
    "\n",
    "    # there are still leading spaces on strings after the split, so trim those off\n",
    "    for element in scraped_categories:\n",
    "        for category in range(len(element)):\n",
    "            if element[category][0] == \" \":\n",
    "                element[category] = element[category][1:]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Categories'] = scraped_categories\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Return the dictionary\n",
    "    return london_yelp_restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=0',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=10',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=20',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=30',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=40',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=50',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=60',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=70',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=80',\n",
       " 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1&start=90']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = \"https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1\"\n",
    "\n",
    "# generate a list like [0, 10, 20, ..., 90] to get the first ten pages (first 100 results)\n",
    "page_start_values = [i * 10 for i in range(0,10)] \n",
    "\n",
    "# create a list of URLs to scrape\n",
    "urls_to_scrape = [f\"{base_url}&start={value}\" for value in page_start_values]   # append a suffix to the base url\n",
    "\n",
    "urls_to_scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dictionary to hold the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create a dictionary to hold the results (a dictionary of lists)\n",
    "london_yelp_restaurants = {\n",
    "    \"Name\": [],\n",
    "    \"Rank\": [],\n",
    "    \"Price range\": [],\n",
    "    \"Star rating\": [],\n",
    "    \"Neighborhood\": [],\n",
    "    \"Categories\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through the pages and call the scraping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's loop through all the pages we want to scrape, and call the function\n",
    "\n",
    "for url in urls_to_scrape:  # for each site in the list of URLs to scrape\n",
    "    # call the scrape function for the page and store it in a dictionary\n",
    "    single_page_yelp_results = scrape_a_page(url)\n",
    "    \n",
    "    # now append the single-page results to the main results dictionary\n",
    "    for key in london_yelp_restaurants:\n",
    "        london_yelp_restaurants[key] += single_page_yelp_results[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the category lists into their own individual columns\n",
    "After we iterate through all the pages, we can find the maximum number of categories we encountered.  We'll make each restaurant's list be the same length by \"padding\" with `None` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the max number of categories\n",
    "max_categories = 0\n",
    "for category_list in london_yelp_restaurants['Categories']:\n",
    "    if len(category_list) > max_categories:\n",
    "        max_categories = len(category_list)\n",
    "        \n",
    "# max_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we pad the shorter lists with `None` values so they're all the same length\n",
    "for category_list in london_yelp_restaurants['Categories']:\n",
    "    if len(category_list) < max_categories:   # if the restaurant's category list is shorter\n",
    "        while len(category_list) < max_categories:\n",
    "            category_list.append(None)    # keep appending None to the end of the category list until it's long enough\n",
    "\n",
    "# london_yelp_restaurants['Categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to break up these equal-length lists into their own categories\n",
    "# to do this, we'll create one new dictionary key for each category number\n",
    "for i in range(max_categories):\n",
    "    column_name = f\"Category_{i + 1}\"\n",
    "    london_yelp_restaurants[column_name] = []\n",
    "    \n",
    "# london_yelp_restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll populate the new category values\n",
    "# we want this to be flexible so any number of categories will work.  \n",
    "# So avoid hard-coding variables like 'Category_2'\n",
    "\n",
    "# iterate through each category list in the dictionary and place the elements into the appropriate individual category key\n",
    "for category_list in london_yelp_restaurants['Categories']:\n",
    "    for category_number in range(max_categories):\n",
    "        london_yelp_restaurants[f\"Category_{category_number + 1}\"].append(category_list[category_number])\n",
    "\n",
    "# london_yelp_restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Fish & Chips', None, None],\n",
       " ['Indian', None, None],\n",
       " ['Steakhouses', None, None],\n",
       " ['British', None, None],\n",
       " ['Indian', None, None],\n",
       " ['Coffee & Tea', 'Breakfast & Brunch', 'American (Traditional)'],\n",
       " ['French', 'British', None],\n",
       " ['American (Traditional)', 'Soul Food', 'Cajun/Creole'],\n",
       " ['Mediterranean', None, None],\n",
       " ['French', 'Modern European', 'Cocktail Bars'],\n",
       " ['Fish & Chips', None, None],\n",
       " ['Burgers', None, None],\n",
       " ['Italian', None, None],\n",
       " ['Mexican', 'Bars', None],\n",
       " ['Taiwanese', None, None],\n",
       " ['British', 'Steakhouses', 'Cocktail Bars'],\n",
       " ['Chinese', 'Noodles', None],\n",
       " ['British', 'Pubs', 'Gastropubs'],\n",
       " ['Modern European', 'Bars', 'British'],\n",
       " ['Middle Eastern', 'Mediterranean', None],\n",
       " ['Mexican', None, None],\n",
       " ['Spanish', 'Tapas Bars', None],\n",
       " ['British', None, None],\n",
       " ['British', 'Cocktail Bars', 'Steakhouses'],\n",
       " ['Cafes', None, None],\n",
       " ['Seafood', 'Burgers', None],\n",
       " ['Pizza', None, None],\n",
       " ['Dim Sum', 'Seafood', 'Noodles'],\n",
       " ['Italian', None, None],\n",
       " ['Japanese', None, None],\n",
       " ['Middle Eastern', None, None],\n",
       " ['Noodles', 'Thai', None],\n",
       " ['Korean', None, None],\n",
       " ['Caribbean', None, None],\n",
       " ['Thai', None, None],\n",
       " ['French', None, None],\n",
       " ['Modern European', None, None],\n",
       " ['Japanese', None, None],\n",
       " ['Korean', None, None],\n",
       " ['Cafes', None, None],\n",
       " ['Cafes', 'British', 'Tea Rooms'],\n",
       " ['Pizza', 'Bars', None],\n",
       " ['Chinese', None, None],\n",
       " ['Japanese', None, None],\n",
       " ['Italian', None, None],\n",
       " ['Tapas/Small Plates', 'Spanish', None],\n",
       " ['Breakfast & Brunch', 'Pizza', None],\n",
       " ['British', 'Coffee & Tea', 'Tea Rooms'],\n",
       " ['Chicken Shop', 'Fast Food', None],\n",
       " ['American (Traditional)', 'Barbeque', 'Burgers'],\n",
       " ['Bars', 'Venues & Event Spaces', 'American (Traditional)'],\n",
       " ['Italian', None, None],\n",
       " ['Pubs', 'British', None],\n",
       " ['Gastropubs', 'British', None],\n",
       " ['Ramen', None, None],\n",
       " ['Bakeries', 'Bistros', None],\n",
       " ['Scandinavian', 'Bakeries', None],\n",
       " ['Pancakes', None, None],\n",
       " ['Sri Lankan', 'Asian Fusion', None],\n",
       " ['Coffee & Tea', 'Breakfast & Brunch', 'Cafes'],\n",
       " ['Japanese', None, None],\n",
       " ['British', None, None],\n",
       " ['American (Traditional)', 'Burgers', 'Seafood'],\n",
       " ['Spanish', None, None],\n",
       " ['Middle Eastern', None, None],\n",
       " ['Italian', None, None],\n",
       " ['Mexican', None, None],\n",
       " ['Steakhouses', None, None],\n",
       " ['Pubs', 'British', None],\n",
       " ['Indian', 'Asian Fusion', 'Singaporean'],\n",
       " ['Indian', 'Pakistani', 'Halal'],\n",
       " ['British', 'Modern European', None],\n",
       " ['Italian', None, None],\n",
       " ['Thai', 'Asian Fusion', 'Vegetarian'],\n",
       " ['Thai', None, None],\n",
       " ['Japanese', 'Sushi Bars', None],\n",
       " ['British', 'Pubs', None],\n",
       " ['Bars', 'Waffles', None],\n",
       " ['Korean', None, None],\n",
       " ['Chicken Shop', 'American (New)', 'Soul Food'],\n",
       " ['Italian', None, None],\n",
       " ['Fish & Chips', None, None],\n",
       " ['Cafes', 'Korean', None],\n",
       " ['Middle Eastern', 'Wine Bars', None],\n",
       " ['Fish & Chips', None, None],\n",
       " ['British', 'Breakfast & Brunch', 'Bars'],\n",
       " ['Mexican', None, None],\n",
       " ['Seafood', 'Steakhouses', None],\n",
       " ['American (Traditional)', 'Seafood', 'Burgers'],\n",
       " ['Coffee & Tea', 'Breakfast & Brunch', None],\n",
       " ['Coffee & Tea', 'Sandwiches', None],\n",
       " ['Chicken Shop', 'Fast Food', None],\n",
       " ['Indian', None, None],\n",
       " ['Modern European', 'British', None],\n",
       " ['Modern European', 'Mediterranean', None],\n",
       " ['Italian', None, None],\n",
       " ['Malaysian', None, None],\n",
       " ['Middle Eastern', 'Mediterranean', None],\n",
       " ['Italian', None, None],\n",
       " ['Belgian', None, None]]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now delete the 'Categories' key/pair value from the dictionary, because we don't need it anymore\n",
    "\n",
    "london_yelp_restaurants.pop('Categories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Results into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Price range</th>\n",
       "      <th>Star rating</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Category_1</th>\n",
       "      <th>Category_2</th>\n",
       "      <th>Category_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Mayfair Chippy</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Mayfair</td>\n",
       "      <td>Fish &amp; Chips</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Covent Garden</td>\n",
       "      <td>Indian</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flat Iron</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Soho</td>\n",
       "      <td>Steakhouses</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ffiona’s Restaurant</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Kensington</td>\n",
       "      <td>British</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Soho</td>\n",
       "      <td>Indian</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Cecconi’s Mayfair</td>\n",
       "      <td>96</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Mayfair</td>\n",
       "      <td>Italian</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Laksamania</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Fitzrovia</td>\n",
       "      <td>Malaysian</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>The Barbary</td>\n",
       "      <td>98</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Covent Garden</td>\n",
       "      <td>Middle Eastern</td>\n",
       "      <td>Mediterranean</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>La Porchetta Pollo Bar</td>\n",
       "      <td>99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Italian</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Belgo Centraal</td>\n",
       "      <td>100</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Covent Garden</td>\n",
       "      <td>Belgian</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name  Rank  Price range  Star rating   Neighborhood  \\\n",
       "0       The Mayfair Chippy     1          2.0          4.5        Mayfair   \n",
       "1                  Dishoom     2          2.0          4.5  Covent Garden   \n",
       "2                Flat Iron     3          2.0          4.5           Soho   \n",
       "3      Ffiona’s Restaurant     4          2.0          4.5     Kensington   \n",
       "4                  Dishoom     5          2.0          4.5           Soho   \n",
       "..                     ...   ...          ...          ...            ...   \n",
       "95       Cecconi’s Mayfair    96          3.0          4.0        Mayfair   \n",
       "96              Laksamania    97          NaN          4.5      Fitzrovia   \n",
       "97             The Barbary    98          3.0          4.5  Covent Garden   \n",
       "98  La Porchetta Pollo Bar    99          1.0          4.0     Bloomsbury   \n",
       "99          Belgo Centraal   100          2.0          4.0  Covent Garden   \n",
       "\n",
       "        Category_1     Category_2 Category_3  \n",
       "0     Fish & Chips           None       None  \n",
       "1           Indian           None       None  \n",
       "2      Steakhouses           None       None  \n",
       "3          British           None       None  \n",
       "4           Indian           None       None  \n",
       "..             ...            ...        ...  \n",
       "95         Italian           None       None  \n",
       "96       Malaysian           None       None  \n",
       "97  Middle Eastern  Mediterranean       None  \n",
       "98         Italian           None       None  \n",
       "99         Belgian           None       None  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and let's load it into a dataframe\n",
    "pd.DataFrame(london_yelp_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier 4 - Cleaned-Up Code Answer\n",
    "Below is my cleaned-up code that executes everything in a single cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Price range</th>\n",
       "      <th>Star rating</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Category_1</th>\n",
       "      <th>Category_2</th>\n",
       "      <th>Category_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Mayfair Chippy</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Mayfair</td>\n",
       "      <td>Fish &amp; Chips</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Covent Garden</td>\n",
       "      <td>Indian</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flat Iron</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Soho</td>\n",
       "      <td>Steakhouses</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ffiona’s Restaurant</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Kensington</td>\n",
       "      <td>British</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Soho</td>\n",
       "      <td>Indian</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Cecconi’s Mayfair</td>\n",
       "      <td>96</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Mayfair</td>\n",
       "      <td>Italian</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Laksamania</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Fitzrovia</td>\n",
       "      <td>Malaysian</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>The Barbary</td>\n",
       "      <td>98</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Covent Garden</td>\n",
       "      <td>Middle Eastern</td>\n",
       "      <td>Mediterranean</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>La Porchetta Pollo Bar</td>\n",
       "      <td>99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Italian</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Belgo Centraal</td>\n",
       "      <td>100</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Covent Garden</td>\n",
       "      <td>Belgian</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name  Rank  Price range  Star rating   Neighborhood  \\\n",
       "0       The Mayfair Chippy     1          2.0          4.5        Mayfair   \n",
       "1                  Dishoom     2          2.0          4.5  Covent Garden   \n",
       "2                Flat Iron     3          2.0          4.5           Soho   \n",
       "3      Ffiona’s Restaurant     4          2.0          4.5     Kensington   \n",
       "4                  Dishoom     5          2.0          4.5           Soho   \n",
       "..                     ...   ...          ...          ...            ...   \n",
       "95       Cecconi’s Mayfair    96          3.0          4.0        Mayfair   \n",
       "96              Laksamania    97          NaN          4.5      Fitzrovia   \n",
       "97             The Barbary    98          3.0          4.5  Covent Garden   \n",
       "98  La Porchetta Pollo Bar    99          1.0          4.0     Bloomsbury   \n",
       "99          Belgo Centraal   100          2.0          4.0  Covent Garden   \n",
       "\n",
       "        Category_1     Category_2 Category_3  \n",
       "0     Fish & Chips           None       None  \n",
       "1           Indian           None       None  \n",
       "2      Steakhouses           None       None  \n",
       "3          British           None       None  \n",
       "4           Indian           None       None  \n",
       "..             ...            ...        ...  \n",
       "95         Italian           None       None  \n",
       "96       Malaysian           None       None  \n",
       "97  Middle Eastern  Mediterranean       None  \n",
       "98         Italian           None       None  \n",
       "99         Belgian           None       None  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're going to need three Python libraries to make this code work:\n",
    "\n",
    "import requests   # this library will help us make http requests, which is how we get webpages\n",
    "from bs4 import BeautifulSoup   # this library will help us parse html source code (i.e., webscraping)\n",
    "import pandas as pd   # this library will help us with data science stuff\n",
    "\n",
    "\n",
    "\n",
    "#### Define the scraping function\n",
    "def scrape_a_page(url_to_scrape):\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Make the Request\n",
    "    # So let's get to requestin'!\n",
    "    http_response = requests.get(url_to_scrape)   # this actually requests the page and stores the resulting response\n",
    "\n",
    "    # get the text version of the http_response\n",
    "    source_code_text = http_response.text   # the .text method is part of the requests library, not BeautifulSoup\n",
    "\n",
    "    # then we'll give that text source code to BeautifulSoup, which creates an object with useful scraping methods\n",
    "    yelp_scrape = BeautifulSoup(source_code_text)\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Initialize the Dictionary of Results\n",
    "    # we're going to structure this data as a dictionary of lists\n",
    "    london_yelp_restaurants = {}\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Names\n",
    "    # let's scrape for names:\n",
    "    scraped_names = yelp_scrape.find_all('a', {'class': 'css-166la90'})  # this returns a list of tags that match the criteria\n",
    "\n",
    "    # get the text\n",
    "    scraped_names = [scraped_name.text for scraped_name in scraped_names]\n",
    "\n",
    "    # clean it up by filtering out empty strings and numbers \n",
    "    # this logic will be a problem if a restaurant's name is just a number...\n",
    "    for name in scraped_names:\n",
    "        scraped_names = [name for name in scraped_names if name != '' and not name.isdigit()]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Name'] = scraped_names\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Ranks\n",
    "    # now let's scrape for ranks:\n",
    "    scraped_ranks = yelp_scrape.find_all('span', {'class': 'css-1pxmz4g'})\n",
    "\n",
    "    # get the text\n",
    "    scraped_ranks = [rank.text for rank in scraped_ranks]  # get the text between the tags\n",
    "\n",
    "    # now parse out the number\n",
    "    scraped_ranks = [rank[0:rank.index('.')] for rank in scraped_ranks]  # grab the first characters before the \".\"\n",
    "\n",
    "    # let's assume we want integers, so convert\n",
    "    scraped_ranks = [int(rank) for rank in scraped_ranks]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Rank'] = scraped_ranks\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Price Ranges\n",
    "    # now let's scrape for price ranges.  This one got tricky, and we had to go for an outside-in approach.\n",
    "    scraped_price_ranges = yelp_scrape.find_all('div', {'class': 'priceCategory__09f24__2IbAM'})\n",
    "\n",
    "    # get the text\n",
    "    scraped_price_ranges = [price.text for price in scraped_price_ranges]\n",
    "\n",
    "    # now grab the £ signs if they're there, otherwise `None`\n",
    "    for element in range(len(scraped_price_ranges)):\n",
    "        if scraped_price_ranges[element].count(\"\\xA3\") == 0:\n",
    "            scraped_price_ranges[element] = None\n",
    "        else:\n",
    "            scraped_price_ranges[element] = scraped_price_ranges[element].count(\"\\xA3\")\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Price range'] = scraped_price_ranges\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Star Ratings\n",
    "    # now let's scrape for star rating:\n",
    "    scraped_star_ratings = yelp_scrape.find_all('div', {'class': 'i-stars__09f24__1T6rz'})\n",
    "\n",
    "    # narrow it down\n",
    "    scraped_star_ratings = [rating.attrs['aria-label'] for rating in scraped_star_ratings]\n",
    "\n",
    "    # clean it up by getting rid of the \"star rating\" suffix and converting to a float\n",
    "    scraped_star_ratings = [float(rating[0:len(rating) - len(\" star rating\")]) for rating in scraped_star_ratings]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Star rating'] = scraped_star_ratings\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Neighborhood\n",
    "    # now let's scrape for neighborhood:\n",
    "    scraped_neighborhood_containers = yelp_scrape.find_all('div', {'class': 'container__09f24__1fWZl'})\n",
    "\n",
    "    # now let's just get the <p> tags within each container\n",
    "    scraped_neighborhoods = [container.find_all('p') for container in scraped_neighborhood_containers]\n",
    "\n",
    "    # let's simplify what we're looking at by getting text of each tag\n",
    "    for collection in scraped_neighborhoods:\n",
    "        for tag in range(0,len(collection)):\n",
    "            collection[tag] = collection[tag].text   # is there a more efficienty way to do this?\n",
    "\n",
    "    # now let's get the last element of each list\n",
    "    scraped_neighborhoods = [location[len(location)-1] for location in scraped_neighborhoods]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Neighborhood'] = scraped_neighborhoods\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Scrape for Categories\n",
    "    # now let's scrape for categories\n",
    "    scraped_categories = yelp_scrape.find_all('p', {'class': 'css-n6i4z7'})\n",
    "\n",
    "    # get the text\n",
    "    scraped_categories = [categories.text for categories in scraped_categories]\n",
    "\n",
    "    # now clean it up!  \n",
    "\n",
    "    # Let's assume for now that the ten restaurant-specific results will be at the top.  This could be problematic later.\n",
    "    scraped_categories = scraped_categories[0:10]    # get the first ten results only\n",
    "\n",
    "    # now clean off any £ symbols (some strings may have none)\n",
    "    for categories in range(len(scraped_categories)):\n",
    "        scraped_categories[categories] = scraped_categories[categories].replace('\\xA3', '')\n",
    "\n",
    "    # now split the remaining strings into lists\n",
    "    for element in range(len(scraped_categories)):\n",
    "        scraped_categories[element] = scraped_categories[element].split(',')\n",
    "\n",
    "    # there are still leading spaces on strings after the split, so trim those off\n",
    "    for element in scraped_categories:\n",
    "        for category in range(len(element)):\n",
    "            if element[category][0] == \" \":\n",
    "                element[category] = element[category][1:]\n",
    "\n",
    "    # add it to the dictionary\n",
    "    london_yelp_restaurants['Categories'] = scraped_categories\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Return the dictionary\n",
    "    return london_yelp_restaurants\n",
    "\n",
    "\n",
    "\n",
    "#### Generate the URLs\n",
    "base_url = \"https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1\"\n",
    "\n",
    "# generate a list like [0, 10, 20, ..., 90] to get the first ten pages (first 100 results)\n",
    "page_start_values = [i * 10 for i in range(0,10)] \n",
    "\n",
    "# create a list of URLs to scrape\n",
    "urls_to_scrape = [f\"{base_url}&start={value}\" for value in page_start_values]   # append a suffix to the base url\n",
    "\n",
    "\n",
    "\n",
    "#### Create a dictionary to hold the results\n",
    "# first, create a dictionary to hold the results (a dictionary of lists)\n",
    "london_yelp_restaurants = {\n",
    "    \"Name\": [],\n",
    "    \"Rank\": [],\n",
    "    \"Price range\": [],\n",
    "    \"Star rating\": [],\n",
    "    \"Neighborhood\": [],\n",
    "    \"Categories\": []\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#### Loop through the pages and call the scraping function\n",
    "# let's loop through all the pages we want to scrape, and call the function\n",
    "\n",
    "for url in urls_to_scrape:  # for each site in the list of URLs to scrape\n",
    "    # call the scrape function for the page and store it in a dictionary\n",
    "    single_page_yelp_results = scrape_a_page(url)\n",
    "    \n",
    "    # now append the single-page results to the main results dictionary\n",
    "    for key in london_yelp_restaurants:\n",
    "        london_yelp_restaurants[key] += single_page_yelp_results[key]\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "#### Split the category lists into their own individual columns\n",
    "# After we iterate through all the pages, we can find the maximum number of categories we encountered\n",
    "# We'll make each restaurant's list be the same length by \"padding\" with `None` values.\n",
    "\n",
    "# find the max number of categories\n",
    "max_categories = 0\n",
    "for category_list in london_yelp_restaurants['Categories']:\n",
    "    if len(category_list) > max_categories:\n",
    "        max_categories = len(category_list)\n",
    "\n",
    "# Now we pad the shorter lists with `None` values so they're all the same length\n",
    "for category_list in london_yelp_restaurants['Categories']:\n",
    "    if len(category_list) < max_categories:   # if the restaurant's category list is shorter\n",
    "        while len(category_list) < max_categories:\n",
    "            category_list.append(None)    # keep appending None to the end of the category list until it's long enough\n",
    "\n",
    "# now we need to break up these equal-length lists into their own categories\n",
    "# to do this, we'll create one new dictionary key for each category number\n",
    "for i in range(max_categories):\n",
    "    column_name = f\"Category_{i + 1}\"\n",
    "    london_yelp_restaurants[column_name] = []\n",
    "\n",
    "# now we'll populate the new category values\n",
    "# we want this to be flexible so any number of categories will work.  \n",
    "# So avoid hard-coding variables like 'Category_2'\n",
    "# iterate through each category list in the dictionary and place the elements into the appropriate individual category key\n",
    "for category_list in london_yelp_restaurants['Categories']:\n",
    "    for category_number in range(max_categories):\n",
    "        london_yelp_restaurants[f\"Category_{category_number + 1}\"].append(category_list[category_number])\n",
    "\n",
    "# now delete the 'Categories' key/pair value from the dictionary, because we don't need it anymore\n",
    "london_yelp_restaurants.pop('Categories')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Load the Results into a DataFrame\n",
    "pd.DataFrame(london_yelp_restaurants)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
