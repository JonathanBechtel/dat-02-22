{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit 1 Homework:  Scraping the Yelp Website\n",
    "\n",
    "Welcome!  For this homework assignment you'll be tasked with building a web scraper in a manner that builds on what was covered in our web scraping class.\n",
    "\n",
    "The assignment will extend the lab work done during that time, where we built a dataset that listed the name, number of reviews and price range for restaurant on the following web page: https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1\n",
    "\n",
    "**What You'll Turn In:**\n",
    "\n",
    "A finished jupyter notebook that walks us through the steps you took in order to get your results.  Provide notes where appropriate to explain what you are doing.\n",
    "\n",
    "The notebook should produce a finished dataset at the end.  \n",
    "\n",
    "If for some reason you're experiencing problems with the final result, please let someone know when turning it in.\n",
    " \n",
    "The homework is divided into five tiers, each of which have increasing levels of difficulty:\n",
    "\n",
    "##### Tier 1: Five Columns From the First Page\n",
    "\n",
    "At the most basic level for this assignment, you will need to extend what we did in class, and create a dataset that has five columns in it that are 30 rows long.  This means you will not need to go off the first page in order to complete this section.\n",
    "\n",
    "##### Tier 2:  100 Row Dataset With At Least 3 Columns\n",
    "\n",
    "For this portion of the assignment, take 3 of your columns from step 1, and extend them out to multiple pages on the yelp website.  You should appropriately account for the presence of missing values.\n",
    "\n",
    "##### Tier 3:  100 Row Dataset With At Least 5 Columns\n",
    "\n",
    "Very similar to Tier 2, but if you use this many columns you will be forced to encounter some columns that will frequently have missing values, whereas with Tier 2 you could likely skip these if you wanted to.  \n",
    "\n",
    "##### Tier 4:  100 Row Dataset With At Least 5 Columns + Individual Restaurant Categories\n",
    "\n",
    "Restaurants often have different categories associated with them, so grabbing them individually as separate values is often challenging.  To complete this tier, you'll have to find a way to 'pick out' each of the individual categories as their own separate column value.  \n",
    "\n",
    "##### Tier 5:  Unlimited Row Dataset With At Least 5 Columns + Individual Restaurant Categories\n",
    "\n",
    "Take what you did in Tier 4, and extend it so that the code will work with an arbitrary number of pages.  Ie, regardless of how many pages there are listing the best restaurants in London, your scraper will find them, and cleanly parse their information into clean datasets.\n",
    "\n",
    "### Hints\n",
    "\n",
    "Here are a few tips that will save you time when completing this assignment:\n",
    "\n",
    " - The name, average rating, total ratings and neighborhood of a restaurant tend to be the 'easy' ones, because they rarely have missing values, so what ever logic you use on the first page will typically apply to all pages.  They are a good place to start\n",
    " - Phone numbers, price ranges and reviews are more commonly missing, so if you are trying to get a larger number of items from them across multiple pages you should expect to do some error handling\n",
    " - You can specify any sort of selector when using the `find_all()` method, not just `class`.  For example, imagine you have the following `<div>` tag:\n",
    "    `<div class='main-container red-blue-green' role='front-unit' aria-select='left-below'>Some content here</div>`\n",
    "    \n",
    "   This means that when you use `scraper.find_all('div')`, you can pass in arguments like `scraper.find_all('div', {'role': 'front-unit'})` or anything else that allows you to isolate that particular tag.\n",
    " - When specifying selectors like `{'class': 'dkght__384Ko'}`, sometimes less is more.  If you include multiple selectors, you are saying return a tag with **any one of these** distinctions, not all of them.  So if your results are large, try different combinations of selectors to get the smallest results possible.\n",
    " - If you begin dealing with values that are unreliably entered, you should use the 'outside in' technique where you grab a parent container that holds the element and find a way to check to see if a particular value is there by scraping it further.  The best way to do this is to try and find a unique container for every single restaurant.  This means that you will have a reliable parent element for every single restaurant, and within *each of these* you can search for `<p>`, `<a>`, `<div>`, and `<span>` tags and apply further logic.\n",
    " - When you get results from `BeautifulSoup`, you will be given data that's denoted as either `bs4.element.Tag` or `bs4.element.ResultSet`.  They are **not the same**.  Critically, you can search a `bs4.element.Tag` for further items, but you cannot do this with a `bs4.element.ResultSet`.  \n",
    " \n",
    "   For example, let's say you grab all of the divs from a page with `scraper.find_all('div')` and save it as the variable `total_divs`.  This means `total_divs` will look somethig like this:  \n",
    "   \n",
    "   `[<div><p>Div content</p><p>Second paragraph</p></div>,`\n",
    "      `<div><p>Div content</p><p>Second paragraph</p></div>,`\n",
    "      `<div><p>Div content</p><p>Second paragraph</p></div>]`\n",
    "      \n",
    "   In this case the variable `total_divs` is a result set and there's nothing else you can do to it directly.  However, every item within `total_divs` is a tag, which means you can scrape it further.  \n",
    "   \n",
    "   So if you wanted you could write a line like:  `total_paragraphs = [div.find_all('p') for div in total_divs]`, and get the collection of paragraphs within each div.  \n",
    "   \n",
    "   If you confuse the two you'll get the following error message:  \n",
    "   \n",
    "   `AttributeError: ResultSet object has no attribute 'find_all'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?`\n",
    " - The values of the different selectors change periodically on yelp, so if your scraper all of a sudden stops working that's probably why.  Ie, if you have a command like `scraper.find_all('div', {'class': '485dk0W__container09'}` that no longer returns results, the class `485dk0W__container09` may now be `r56kW__container14` or something similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py:981: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.yelp.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url='https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C+United+Kingdom&ns=1'\n",
    "yelp_req = requests.get(url, verify=False)\n",
    "scraper = BeautifulSoup(yelp_req.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['282', '1842', '267', '380', '204', '494', '122', '206', '106', '547']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting first column Review\n",
    "reviews = scraper.find_all('span', {'class': 'reviewCount__09f24__EUXPN css-e81eai'})\n",
    "reviews = [rs.text for rs in reviews]\n",
    "reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['££', '££', '££', '££', '££££', '££', '££', '££', '££', '££']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting second column Price Range\n",
    "priceRange = scraper.find_all('span', {'class': 'priceRange__09f24__2O6le css-xtpg8e'})\n",
    "priceRange = [rs.text for rs in priceRange]\n",
    "priceRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.\\xa0The Mayfair Chippy',\n",
       " '2.\\xa0Dishoom',\n",
       " '3.\\xa0Ffiona’s Restaurant',\n",
       " '4.\\xa0Flat Iron',\n",
       " '5.\\xa0Restaurant Gordon Ramsay',\n",
       " '6.\\xa0The Breakfast Club',\n",
       " '7.\\xa0The Fat Bear',\n",
       " '8.\\xa0Padella',\n",
       " '9.\\xa0The Golden Chippy',\n",
       " '10.\\xa0Dishoom']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting second column Price Range\n",
    "titles = scraper.find_all('span', {'class': 'css-1pxmz4g'})\n",
    "titles = [rs.text for rs in titles]\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_titles = titles\n",
    "clean_titles = [s[s.find('\\xa0'):] for s in titles]\n",
    "clean_titles = [rs.replace('\\xa0','') for rs in clean_titles]\n",
    "clean_titles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>NumReviews</th>\n",
       "      <th>PriceRange</th>\n",
       "      <th>Streets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Mayfair Chippy</td>\n",
       "      <td>282</td>\n",
       "      <td>££</td>\n",
       "      <td>14 North Audley Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>1842</td>\n",
       "      <td>££</td>\n",
       "      <td>12 Upper Saint Martin's Lane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ffiona’s Restaurant</td>\n",
       "      <td>267</td>\n",
       "      <td>££</td>\n",
       "      <td>51 Kensington Church Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flat Iron</td>\n",
       "      <td>380</td>\n",
       "      <td>££</td>\n",
       "      <td>17 Beak Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Restaurant Gordon Ramsay</td>\n",
       "      <td>204</td>\n",
       "      <td>££££</td>\n",
       "      <td>68 Royal Hospital Road</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Breakfast Club</td>\n",
       "      <td>494</td>\n",
       "      <td>££</td>\n",
       "      <td>33 D'Arblay Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Fat Bear</td>\n",
       "      <td>122</td>\n",
       "      <td>££</td>\n",
       "      <td>61 Carter Lane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Padella</td>\n",
       "      <td>206</td>\n",
       "      <td>££</td>\n",
       "      <td>6 Southwark Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Golden Chippy</td>\n",
       "      <td>106</td>\n",
       "      <td>££</td>\n",
       "      <td>62 Greenwich High Road</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>547</td>\n",
       "      <td>££</td>\n",
       "      <td>22 Kingly Street</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name NumReviews PriceRange  \\\n",
       "0        The Mayfair Chippy        282         ££   \n",
       "1                   Dishoom       1842         ££   \n",
       "2       Ffiona’s Restaurant        267         ££   \n",
       "3                 Flat Iron        380         ££   \n",
       "4  Restaurant Gordon Ramsay        204       ££££   \n",
       "5        The Breakfast Club        494         ££   \n",
       "6              The Fat Bear        122         ££   \n",
       "7                   Padella        206         ££   \n",
       "8         The Golden Chippy        106         ££   \n",
       "9                   Dishoom        547         ££   \n",
       "\n",
       "                        Streets  \n",
       "0        14 North Audley Street  \n",
       "1  12 Upper Saint Martin's Lane  \n",
       "2   51 Kensington Church Street  \n",
       "3                17 Beak Street  \n",
       "4        68 Royal Hospital Road  \n",
       "5            33 D'Arblay Street  \n",
       "6                61 Carter Lane  \n",
       "7            6 Southwark Street  \n",
       "8        62 Greenwich High Road  \n",
       "9              22 Kingly Street  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Streets = scraper.find_all('span', {'class': 'raw__09f24__3Obuy'})\n",
    "Streets = [rs.text for rs in Streets]\n",
    "Streets = [rss for rss in Streets if rss[:1].isnumeric()]\n",
    "Streets\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df_dict = {\n",
    "    'Name': clean_titles,\n",
    "    'NumReviews': reviews,\n",
    "    'PriceRange': priceRange,\n",
    "    'Streets': Streets\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(df_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Soho',\n",
       " 'Mayfair',\n",
       " 'Covent Garden',\n",
       " 'Soho',\n",
       " 'Kensington',\n",
       " 'Soho',\n",
       " 'Chelsea',\n",
       " 'Blackfriars',\n",
       " 'Soho',\n",
       " 'Soho']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting second column Price Range\n",
    "#category = scraper.find_all('a', {'class': 'css-1joxor6'})\n",
    "#category = scraper.find_all('span', {'class': 'css-f7rmk3'})\n",
    "category = scraper.find_all('span', {'class': 'display--inline__09f24__EhyFv border-color--default__09f24__1eOd'})\n",
    "\n",
    "category = [rs.text for rs in category]\n",
    "category\n",
    "\n",
    "cities = scraper.find_all('p', {'class': 'css-8jxw1i'})\n",
    "cities = [rs.text for rs in cities]\n",
    "cities = [rss for rss in cities if rss[:1].isalpha()]\n",
    "cities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py:981: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.yelp.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py:981: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.yelp.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py:981: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.yelp.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titles - 30  review - 30  Price - 30  street - 30  city - 30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>NumReviews</th>\n",
       "      <th>PriceRange</th>\n",
       "      <th>Streets</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Breakfast Club</td>\n",
       "      <td>494</td>\n",
       "      <td>££</td>\n",
       "      <td>33 D'Arblay Street</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Mayfair Chippy</td>\n",
       "      <td>282</td>\n",
       "      <td>££</td>\n",
       "      <td>14 North Audley Street</td>\n",
       "      <td>Mayfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>1841</td>\n",
       "      <td>££</td>\n",
       "      <td>12 Upper Saint Martin's Lane</td>\n",
       "      <td>Covent Garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flat Iron</td>\n",
       "      <td>380</td>\n",
       "      <td>££</td>\n",
       "      <td>17 Beak Street</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ffiona’s Restaurant</td>\n",
       "      <td>267</td>\n",
       "      <td>££</td>\n",
       "      <td>51 Kensington Church Street</td>\n",
       "      <td>Kensington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dishoom</td>\n",
       "      <td>547</td>\n",
       "      <td>££</td>\n",
       "      <td>22 Kingly Street</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Restaurant Gordon Ramsay</td>\n",
       "      <td>204</td>\n",
       "      <td>££££</td>\n",
       "      <td>68 Royal Hospital Road</td>\n",
       "      <td>Chelsea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Fat Bear</td>\n",
       "      <td>122</td>\n",
       "      <td>££</td>\n",
       "      <td>61 Carter Lane</td>\n",
       "      <td>Blackfriars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mother Mash</td>\n",
       "      <td>470</td>\n",
       "      <td>££</td>\n",
       "      <td>26 Ganton Street</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NOPI</td>\n",
       "      <td>272</td>\n",
       "      <td>£££</td>\n",
       "      <td>21-22 Warwick Street</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sketch</td>\n",
       "      <td>826</td>\n",
       "      <td>£££</td>\n",
       "      <td>9 Conduit Street</td>\n",
       "      <td>Mayfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Padella</td>\n",
       "      <td>206</td>\n",
       "      <td>££</td>\n",
       "      <td>6 Southwark Street</td>\n",
       "      <td>London Bridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Regency Café</td>\n",
       "      <td>392</td>\n",
       "      <td>£</td>\n",
       "      <td>17-19 Regency Street</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Golden Chippy</td>\n",
       "      <td>106</td>\n",
       "      <td>££</td>\n",
       "      <td>62 Greenwich High Road</td>\n",
       "      <td>Deptford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Duck &amp; Waffle</td>\n",
       "      <td>705</td>\n",
       "      <td>£££</td>\n",
       "      <td>110 Bishopsgate</td>\n",
       "      <td>Aldgate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The English Rose Cafe and Tea Shop</td>\n",
       "      <td>213</td>\n",
       "      <td>££</td>\n",
       "      <td>4 Lower Grosvenor Place</td>\n",
       "      <td>Victoria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Mestizo</td>\n",
       "      <td>152</td>\n",
       "      <td>££</td>\n",
       "      <td>103 Hampstead Road</td>\n",
       "      <td>Euston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The Queens Arms</td>\n",
       "      <td>120</td>\n",
       "      <td>££</td>\n",
       "      <td>11 Warwick Way</td>\n",
       "      <td>Victoria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Kennington Lane Cafe</td>\n",
       "      <td>96</td>\n",
       "      <td>£</td>\n",
       "      <td>383 Kennington Lane</td>\n",
       "      <td>Vauxhall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Honey &amp; Co</td>\n",
       "      <td>110</td>\n",
       "      <td>££</td>\n",
       "      <td>25a Warren Street</td>\n",
       "      <td>Fitzrovia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Lanzhou Noodle Bar</td>\n",
       "      <td>351</td>\n",
       "      <td>£</td>\n",
       "      <td>33 Cranbourne Street</td>\n",
       "      <td>Covent Garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BAO - Soho</td>\n",
       "      <td>184</td>\n",
       "      <td>££</td>\n",
       "      <td>53 Lexington Street</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Yauatcha</td>\n",
       "      <td>480</td>\n",
       "      <td>£££</td>\n",
       "      <td>15-17 Broadwick Street</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Abeno</td>\n",
       "      <td>101</td>\n",
       "      <td>££</td>\n",
       "      <td>47 Museum Street</td>\n",
       "      <td>Bloomsbury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Honest Burgers Meard St - Soho</td>\n",
       "      <td>278</td>\n",
       "      <td>££</td>\n",
       "      <td>4a Meard St</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Hawksmoor Seven Dials</td>\n",
       "      <td>342</td>\n",
       "      <td>£££</td>\n",
       "      <td>11 Langley Street</td>\n",
       "      <td>Covent Garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Soho Joe</td>\n",
       "      <td>27</td>\n",
       "      <td>££</td>\n",
       "      <td>34 D'Arblay St</td>\n",
       "      <td>Soho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Gordon Ramsay Street Pizza</td>\n",
       "      <td>30</td>\n",
       "      <td>£££</td>\n",
       "      <td>10 Bread Street</td>\n",
       "      <td>The City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The Palomar Restaurant</td>\n",
       "      <td>104</td>\n",
       "      <td>£££</td>\n",
       "      <td>34 Rupert Street</td>\n",
       "      <td>Chinatown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The Wolseley</td>\n",
       "      <td>552</td>\n",
       "      <td>None</td>\n",
       "      <td>160 Picadilly</td>\n",
       "      <td>Mayfair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Name NumReviews PriceRange  \\\n",
       "0                   The Breakfast Club        494         ££   \n",
       "1                   The Mayfair Chippy        282         ££   \n",
       "2                              Dishoom       1841         ££   \n",
       "3                            Flat Iron        380         ££   \n",
       "4                  Ffiona’s Restaurant        267         ££   \n",
       "5                              Dishoom        547         ££   \n",
       "6             Restaurant Gordon Ramsay        204       ££££   \n",
       "7                         The Fat Bear        122         ££   \n",
       "8                          Mother Mash        470         ££   \n",
       "9                                 NOPI        272        £££   \n",
       "10                              Sketch        826        £££   \n",
       "11                             Padella        206         ££   \n",
       "12                        Regency Café        392          £   \n",
       "13                   The Golden Chippy        106         ££   \n",
       "14                       Duck & Waffle        705        £££   \n",
       "15  The English Rose Cafe and Tea Shop        213         ££   \n",
       "16                             Mestizo        152         ££   \n",
       "17                     The Queens Arms        120         ££   \n",
       "18                Kennington Lane Cafe         96          £   \n",
       "19                          Honey & Co        110         ££   \n",
       "20                  Lanzhou Noodle Bar        351          £   \n",
       "21                          BAO - Soho        184         ££   \n",
       "22                            Yauatcha        480        £££   \n",
       "23                               Abeno        101         ££   \n",
       "24      Honest Burgers Meard St - Soho        278         ££   \n",
       "25               Hawksmoor Seven Dials        342        £££   \n",
       "26                            Soho Joe         27         ££   \n",
       "27          Gordon Ramsay Street Pizza         30        £££   \n",
       "28              The Palomar Restaurant        104        £££   \n",
       "29                        The Wolseley        552       None   \n",
       "\n",
       "                         Streets           City  \n",
       "0             33 D'Arblay Street           Soho  \n",
       "1         14 North Audley Street        Mayfair  \n",
       "2   12 Upper Saint Martin's Lane  Covent Garden  \n",
       "3                 17 Beak Street           Soho  \n",
       "4    51 Kensington Church Street     Kensington  \n",
       "5               22 Kingly Street           Soho  \n",
       "6         68 Royal Hospital Road        Chelsea  \n",
       "7                 61 Carter Lane    Blackfriars  \n",
       "8               26 Ganton Street           Soho  \n",
       "9           21-22 Warwick Street           Soho  \n",
       "10              9 Conduit Street        Mayfair  \n",
       "11            6 Southwark Street  London Bridge  \n",
       "12          17-19 Regency Street    Westminster  \n",
       "13        62 Greenwich High Road       Deptford  \n",
       "14               110 Bishopsgate        Aldgate  \n",
       "15       4 Lower Grosvenor Place       Victoria  \n",
       "16            103 Hampstead Road         Euston  \n",
       "17                11 Warwick Way       Victoria  \n",
       "18           383 Kennington Lane       Vauxhall  \n",
       "19             25a Warren Street      Fitzrovia  \n",
       "20          33 Cranbourne Street  Covent Garden  \n",
       "21           53 Lexington Street           Soho  \n",
       "22        15-17 Broadwick Street           Soho  \n",
       "23              47 Museum Street     Bloomsbury  \n",
       "24                   4a Meard St           Soho  \n",
       "25             11 Langley Street  Covent Garden  \n",
       "26                34 D'Arblay St           Soho  \n",
       "27               10 Bread Street       The City  \n",
       "28              34 Rupert Street      Chinatown  \n",
       "29                 160 Picadilly        Mayfair  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_ranges = [0, 10, 20]\n",
    "titles_list = []\n",
    "price_ranges = []\n",
    "num_reviews  = []\n",
    "street_list = []\n",
    "city_list = []\n",
    "\n",
    "for i in page_ranges:\n",
    "    url = f'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=11&start={i}'\n",
    "    req = requests.get(url, verify=False).text\n",
    "    scraper = BeautifulSoup(req)\n",
    "    p_ranges = scraper.find_all('span', {'class': 'css-xtpg8e'})\n",
    "    p_ranges = [price.text for price in p_ranges]\n",
    "    price_ranges += p_ranges\n",
    "    reviews = scraper.find_all('span', {'class': 'reviewCount__09f24__EUXPN'})\n",
    "    reviews = [review.text for review in reviews]\n",
    "    num_reviews += reviews \n",
    "    titles = scraper.find_all('span', {'class': 'css-1pxmz4g'})\n",
    "    titles = [rs.text for rs in titles]\n",
    "    titles = [s[s.find('\\xa0'):] for s in titles]\n",
    "    titles = [rs.replace('\\xa0','') for rs in titles]\n",
    "    titles_list += titles\n",
    "    Streets = scraper.find_all('span', {'class': 'raw__09f24__3Obuy'})\n",
    "    Streets = [rs.text for rs in Streets]\n",
    "    Streets = [rss for rss in Streets if rss[:1].isnumeric()]\n",
    "    street_list += Streets\n",
    "\n",
    "    cities = scraper.find_all('p', {'class': 'css-8jxw1i'})\n",
    "    cities = [rs.text for rs in cities]\n",
    "    cities = [rss for rss in cities if rss[:1].isalpha()]\n",
    "    city_list += cities\n",
    "\n",
    "price_ranges.append(None)\n",
    "print(f'titles - {len(titles_list)}  review - {len(num_reviews)}  Price - {len(price_ranges)}  street - {len(street_list)}  city - {len(city_list)}')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df_dict = {\n",
    "    'Name': titles_list,\n",
    "    'NumReviews': num_reviews,\n",
    "    'PriceRange': price_ranges,\n",
    "    'Streets': street_list,\n",
    "    'City': city_list\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(df_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
